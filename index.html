<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LNdoremi">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="LNdoremi">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LNdoremi">
  <link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>LNdoremi</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LNdoremi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/06/bert微调最佳实践/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2020/08/06/bert微调最佳实践/" class="post-title-link" itemprop="url">bert微调最佳实践</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-08-06 10:10:54 / Modified: 10:18:28" itemprop="dateCreated datePublished" datetime="2020-08-06T10:10:54+08:00">2020-08-06</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、微调（fine-tuning）的策略"><a href="#一、微调（fine-tuning）的策略" class="headerlink" title="一、微调（fine-tuning）的策略"></a>一、微调（fine-tuning）的策略</h1><ol>
<li>max_sequence_length怎么取？<br>—— head+tail (对于长文本，保留前128个字符和后382个字符)</li>
<li>layer 怎么选？<br>—— 最后一层输出的效果最好</li>
<li>optimizer &amp; learning rate ?<br>—— adam &amp; decay=0.95 &amp; lr=2e-5<h1 id="二、进一步的预训练（Further-pretraining）的策略"><a href="#二、进一步的预训练（Further-pretraining）的策略" class="headerlink" title="二、进一步的预训练（Further pretraining）的策略"></a>二、进一步的预训练（Further pretraining）的策略</h1></li>
<li>within task pretraining (在训练语料上进行预训练)</li>
<li>in-domain pretraining (同一领域上的语料进行预训练) </li>
<li>cross-domain pretraining (在不同领域上的语料进行预训练)<h3 id="实验结论："><a href="#实验结论：" class="headerlink" title="实验结论："></a>实验结论：</h3></li>
<li>steps： 100k 能达到最优的效果</li>
<li>in-domain 优于 within-task</li>
<li>cross-domain 没有明显提升效果（bert-base-uncased 模型训练数据原本就是general domain，因此cross-domain效果并不大）<h1 id="三、不同的方法微调bert模型结果"><a href="#三、不同的方法微调bert模型结果" class="headerlink" title="三、不同的方法微调bert模型结果"></a>三、不同的方法微调bert模型结果</h1>bert-in-domain-pretrain-fine-tune &gt; bert-within-task-pretrain-fine-tune &gt; bert-fine-tune &gt; bert-feature &gt; NN</li>
</ol>
<h3 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h3><ol>
<li>further-pretraining 之后再进行fine-tuning的效果优于 仅仅 fine-tuning；</li>
<li>bert-feature是使用bert模型抽取特征后，作为词向量输入下游任务；</li>
<li>NN指的是cnn，rnn，lstm，gru，attention等<h1 id="四、bert-base-VS-bert-large"><a href="#四、bert-base-VS-bert-large" class="headerlink" title="四、bert base VS bert large"></a>四、bert base VS bert large</h1></li>
<li>bert large 效果优于bert base</li>
<li>公司机器跑不动，out-of-memory</li>
</ol>
<h1 id="五、bert模型微调最佳实践"><a href="#五、bert模型微调最佳实践" class="headerlink" title="五、bert模型微调最佳实践"></a>五、bert模型微调最佳实践</h1><p>第一步：in-domain further pretraining</p>
<p>第二步：fine-tuning</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/03/kappa值的计算方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2020/07/03/kappa值的计算方法/" class="post-title-link" itemprop="url">kappa值的计算方法</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-07-03 09:11:12 / Modified: 09:12:06" itemprop="dateCreated datePublished" datetime="2020-07-03T09:11:12+08:00">2020-07-03</time>
            </span>
          
            

            
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cohen_kappa_score</span><span class="params">(y1, y2, labels=None, weights=None, sample_weight=None)</span>:</span></span><br><span class="line">    <span class="string">r"""Cohen's kappa: a statistic that measures inter-annotator agreement.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This function computes Cohen's kappa [1]_, a score that expresses the level</span></span><br><span class="line"><span class="string">    of agreement between two annotators on a classification problem. It is</span></span><br><span class="line"><span class="string">    defined as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \kappa = (p_o - p_e) / (1 - p_e)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`p_o` is the empirical probability of agreement on the label</span></span><br><span class="line"><span class="string">    assigned to any sample (the observed agreement ratio), and :math:`p_e` is</span></span><br><span class="line"><span class="string">    the expected agreement when both annotators assign labels randomly.</span></span><br><span class="line"><span class="string">    :math:`p_e` is estimated using a per-annotator empirical prior over the</span></span><br><span class="line"><span class="string">    class labels [2]_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Read more in the :ref:`User Guide &lt;cohen_kappa&gt;`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y1 : array, shape = [n_samples]</span></span><br><span class="line"><span class="string">        Labels assigned by the first annotator.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    y2 : array, shape = [n_samples]</span></span><br><span class="line"><span class="string">        Labels assigned by the second annotator. The kappa statistic is</span></span><br><span class="line"><span class="string">        symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    labels : array, shape = [n_classes], optional</span></span><br><span class="line"><span class="string">        List of labels to index the matrix. This may be used to select a</span></span><br><span class="line"><span class="string">        subset of labels. If None, all labels that appear at least once in</span></span><br><span class="line"><span class="string">        ``y1`` or ``y2`` are used.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    weights : str, optional</span></span><br><span class="line"><span class="string">        Weighting type to calculate the score. None means no weighted;</span></span><br><span class="line"><span class="string">        "linear" means linear weighted; "quadratic" means quadratic weighted.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sample_weight : array-like of shape (n_samples,), default=None</span></span><br><span class="line"><span class="string">        Sample weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    kappa : float</span></span><br><span class="line"><span class="string">        The kappa statistic, which is a number between -1 and 1. The maximum</span></span><br><span class="line"><span class="string">        value means complete agreement; zero or lower means chance agreement.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    References</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    .. [1] J. Cohen (1960). "A coefficient of agreement for nominal scales".</span></span><br><span class="line"><span class="string">           Educational and Psychological Measurement 20(1):37-46.</span></span><br><span class="line"><span class="string">           doi:10.1177/001316446002000104.</span></span><br><span class="line"><span class="string">    .. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for</span></span><br><span class="line"><span class="string">           computational linguistics". Computational Linguistics 34(4):555-596.</span></span><br><span class="line"><span class="string">           &lt;https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2&gt;`_</span></span><br><span class="line"><span class="string">    .. [3] `Wikipedia entry for the Cohen's kappa.</span></span><br><span class="line"><span class="string">            &lt;https://en.wikipedia.org/wiki/Cohen%27s_kappa&gt;`_</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    confusion = confusion_matrix(y1, y2, labels=labels,</span><br><span class="line">                                 sample_weight=sample_weight)</span><br><span class="line">    n_classes = confusion.shape[<span class="number">0</span>]</span><br><span class="line">    sum0 = np.sum(confusion, axis=<span class="number">0</span>)</span><br><span class="line">    sum1 = np.sum(confusion, axis=<span class="number">1</span>)</span><br><span class="line">    expected = np.outer(sum0, sum1) / np.sum(sum0)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> weights <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        w_mat = np.ones([n_classes, n_classes], dtype=np.int)</span><br><span class="line">        w_mat.flat[:: n_classes + <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> weights == <span class="string">"linear"</span> <span class="keyword">or</span> weights == <span class="string">"quadratic"</span>:</span><br><span class="line">        w_mat = np.zeros([n_classes, n_classes], dtype=np.int)</span><br><span class="line">        w_mat += np.arange(n_classes)</span><br><span class="line">        <span class="keyword">if</span> weights == <span class="string">"linear"</span>:</span><br><span class="line">            w_mat = np.abs(w_mat - w_mat.T)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            w_mat = (w_mat - w_mat.T) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Unknown kappa weighting type."</span>)</span><br><span class="line"></span><br><span class="line">    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - k</span><br></pre></td></tr></table></figure>
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/12/copy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2020/06/12/copy/" class="post-title-link" itemprop="url">python copy & deepcopy</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-06-12 09:38:43 / Modified: 11:14:35" itemprop="dateCreated datePublished" datetime="2020-06-12T09:38:43+08:00">2020-06-12</time>
            </span>
          
            

            
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Python中的赋值、浅拷贝、深拷贝</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">orig = [<span class="string">'a'</span>, [<span class="string">'a'</span>, <span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值</span></span><br><span class="line">assign = orig</span><br><span class="line"><span class="comment"># 浅拷贝</span></span><br><span class="line">shallow_copy = copy.copy(orig)</span><br><span class="line"><span class="comment"># 深拷贝</span></span><br><span class="line">deep_copy = copy.deepcopy(orig)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'======================================='</span>)</span><br><span class="line">print(<span class="string">'原地址:'</span>, id(orig))</span><br><span class="line">print(<span class="string">'赋值地址:'</span>, id(assign))</span><br><span class="line">print(<span class="string">'浅拷贝地址:'</span>, id(shallow_copy))</span><br><span class="line">print(<span class="string">'深拷贝地址:'</span>, id(deep_copy))</span><br><span class="line">print(<span class="string">'======================================='</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'原对象的对象的地址:'</span>, id(orig[<span class="number">0</span>]), id(orig[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'浅拷贝 对象的对象的地址:'</span>, id(shallow_copy[<span class="number">0</span>]), id(shallow_copy[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'深拷贝 对象的对象的地址:'</span>, id(deep_copy[<span class="number">0</span>]), id(deep_copy[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'======================================='</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'改变原对象的值'</span>)</span><br><span class="line">orig[<span class="number">0</span>] = <span class="string">'b'</span></span><br><span class="line">print(<span class="string">'赋值的值:'</span>, assign)</span><br><span class="line">print(<span class="string">'浅拷贝的值:'</span>, shallow_copy)</span><br><span class="line">print(<span class="string">'深拷贝的值:'</span>, deep_copy)</span><br><span class="line">print(<span class="string">'======================================='</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'改变原对象的对象'</span>)</span><br><span class="line">orig[<span class="number">1</span>][<span class="number">0</span>] = <span class="string">'b'</span></span><br><span class="line">print(<span class="string">'赋值的值:'</span>, assign)</span><br><span class="line">print(<span class="string">'浅拷贝的值:'</span>, shallow_copy)</span><br><span class="line">print(<span class="string">'深拷贝的值:'</span>, deep_copy)</span><br><span class="line">print(<span class="string">'======================================='</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">打印结果：</span><br><span class="line">=======================================</span><br><span class="line">原地址:2421034476680</span><br><span class="line">赋值地址:2421034476680</span><br><span class="line">浅拷贝地址:2421034477640</span><br><span class="line">深拷贝地址:2421034477576</span><br><span class="line">=======================================</span><br><span class="line">原对象的对象的地址:2421031698024 2421034450632</span><br><span class="line">浅拷贝对象的对象的地址:2421031698024 2421034450632</span><br><span class="line">深拷贝对象的对象的地址:2421031698024 2421034478152</span><br><span class="line">=======================================</span><br><span class="line">改变原对象的值</span><br><span class="line">赋值的值: [&apos;b&apos;, [&apos;a&apos;, 2]]</span><br><span class="line">浅拷贝的值: [&apos;a&apos;, [&apos;a&apos;, 2]]</span><br><span class="line">深拷贝的值: [&apos;a&apos;, [&apos;a&apos;, 2]]</span><br><span class="line">=======================================</span><br><span class="line">改变原对象的对象</span><br><span class="line">赋值的值: [&apos;b&apos;, [&apos;b&apos;, 2]]</span><br><span class="line">浅拷贝的值: [&apos;a&apos;, [&apos;b&apos;, 2]]</span><br><span class="line">深拷贝的值: [&apos;a&apos;, [&apos;a&apos;, 2]]</span><br><span class="line">=======================================</span><br></pre></td></tr></table></figure>

<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ol>
<li>赋值：a和b指向同一个对象；</li>
<li>浅拷贝：拷贝父对象，不会拷贝对象的内部的子对象，a和b是两个独立的对象，但是他们的子对象还是指向同一对象；</li>
<li>深拷贝：完全拷贝了父对象及其子对象，完全独立。</li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/07/nlp有用的工具包/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2020/01/07/nlp有用的工具包/" class="post-title-link" itemprop="url">nlp有用的工具包</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-01-07 16:07:55 / Modified: 16:38:47" itemprop="dateCreated datePublished" datetime="2020-01-07T16:07:55+08:00">2020-01-07</time>
            </span>
          
            

            
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>pypinyin</strong><br>将中文转换成拼音</p>
<p><strong>enchant</strong><br>检查英文拼写错误</p>
<p><strong>spacy</strong><br>英文分词</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/07/python常用函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2020/01/07/python常用函数/" class="post-title-link" itemprop="url">python常用函数</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-01-07 16:03:07" itemprop="dateCreated datePublished" datetime="2020-01-07T16:03:07+08:00">2020-01-07</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-15 18:06:56" itemprop="dateModified" datetime="2020-01-15T18:06:56+08:00">2020-01-15</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted(iterable, cmp=None, key=None, reverse=False)</span><br></pre></td></tr></table></figure>

<ul>
<li>iterable 可迭代的对象</li>
<li>cmp 比较函数</li>
<li>key 用来进行比较的元素</li>
<li>reverse 默认False(升序)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lambda [arguments]: expression</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lambda</span><span class="params">(arguments)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> expression</span><br></pre></td></tr></table></figure>

<p><strong>字典排序(按key)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted(d.items(), key=<span class="keyword">lambda</span> s: s[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>字典排序(按值)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted(d.items(), key=<span class="keyword">lambda</span> s: s[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="随机"><a href="#随机" class="headerlink" title="随机"></a>随机</h3><p><strong>列表随机抽取</strong><br>方法是调用<strong>random</strong>, 先打乱顺序(<strong>shuffle</strong>), 再取前n个需要的元素.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">a = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">11</span>,<span class="number">13</span>,<span class="number">15</span>]</span><br><span class="line">random.shuffle(a)</span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="comment">#[13,3,5]</span></span><br></pre></td></tr></table></figure>

<p><strong>字典随机抽取</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">dicta = &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>, <span class="string">'d'</span>: <span class="number">4</span>, <span class="string">'e'</span>: <span class="number">5</span>, <span class="string">'f'</span>: <span class="number">6</span>&#125;</span><br><span class="line"><span class="comment"># 抽取键</span></span><br><span class="line">keys_list = random.sample(dicta.keys(), <span class="number">2</span>)</span><br><span class="line">print(keys_list)</span><br><span class="line"><span class="comment">#['c', 'a']</span></span><br><span class="line"><span class="comment"># 抽取值</span></span><br><span class="line">val_list = random.sample(dicta.items(), <span class="number">2</span>)</span><br><span class="line">print(val_list)</span><br><span class="line"><span class="comment"># [('e', 5), ('a', 1)]</span></span><br></pre></td></tr></table></figure>
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/23/pytorch-tensor-常用运算/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/23/pytorch-tensor-常用运算/" class="post-title-link" itemprop="url">pytorch-tensor-常用运算</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-23 17:48:35" itemprop="dateCreated datePublished" datetime="2019-12-23T17:48:35+08:00">2019-12-23</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-07 16:20:27" itemprop="dateModified" datetime="2020-01-07T16:20:27+08:00">2020-01-07</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/17/word2vec的实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/17/word2vec的实现/" class="post-title-link" itemprop="url">word2vec的实现</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-17 18:23:26 / Modified: 18:24:44" itemprop="dateCreated datePublished" datetime="2019-12-17T18:23:26+08:00">2019-12-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This is a implementation of Word2Vec using numpy. Uncomment the print functions to see Word2Vec in action! Also remember to change the number of epochs and set training_data to training_data[0] to avoid flooding your terminal. A Google Sheet implementation of Word2Vec is also available here - https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing</span></span><br><span class="line"><span class="string">Have fun learning!</span></span><br><span class="line"><span class="string">Author: Derek Chia</span></span><br><span class="line"><span class="string">Email: derek@derekchia.com</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment">## Randomly initialise</span></span><br><span class="line">getW1 = [[<span class="number">0.236</span>, <span class="number">-0.962</span>, <span class="number">0.686</span>, <span class="number">0.785</span>, <span class="number">-0.454</span>, <span class="number">-0.833</span>, <span class="number">-0.744</span>, <span class="number">0.677</span>, <span class="number">-0.427</span>, <span class="number">-0.066</span>],</span><br><span class="line">        [<span class="number">-0.907</span>, <span class="number">0.894</span>, <span class="number">0.225</span>, <span class="number">0.673</span>, <span class="number">-0.579</span>, <span class="number">-0.428</span>, <span class="number">0.685</span>, <span class="number">0.973</span>, <span class="number">-0.070</span>, <span class="number">-0.811</span>],</span><br><span class="line">        [<span class="number">-0.576</span>, <span class="number">0.658</span>, <span class="number">-0.582</span>, <span class="number">-0.112</span>, <span class="number">0.662</span>, <span class="number">0.051</span>, <span class="number">-0.401</span>, <span class="number">-0.921</span>, <span class="number">-0.158</span>, <span class="number">0.529</span>],</span><br><span class="line">        [<span class="number">0.517</span>, <span class="number">0.436</span>, <span class="number">0.092</span>, <span class="number">-0.835</span>, <span class="number">-0.444</span>, <span class="number">-0.905</span>, <span class="number">0.879</span>, <span class="number">0.303</span>, <span class="number">0.332</span>, <span class="number">-0.275</span>],</span><br><span class="line">        [<span class="number">0.859</span>, <span class="number">-0.890</span>, <span class="number">0.651</span>, <span class="number">0.185</span>, <span class="number">-0.511</span>, <span class="number">-0.456</span>, <span class="number">0.377</span>, <span class="number">-0.274</span>, <span class="number">0.182</span>, <span class="number">-0.237</span>],</span><br><span class="line">        [<span class="number">0.368</span>, <span class="number">-0.867</span>, <span class="number">-0.301</span>, <span class="number">-0.222</span>, <span class="number">0.630</span>, <span class="number">0.808</span>, <span class="number">0.088</span>, <span class="number">-0.902</span>, <span class="number">-0.450</span>, <span class="number">-0.408</span>],</span><br><span class="line">        [<span class="number">0.728</span>, <span class="number">0.277</span>, <span class="number">0.439</span>, <span class="number">0.138</span>, <span class="number">-0.943</span>, <span class="number">-0.409</span>, <span class="number">0.687</span>, <span class="number">-0.215</span>, <span class="number">-0.807</span>, <span class="number">0.612</span>],</span><br><span class="line">        [<span class="number">0.593</span>, <span class="number">-0.699</span>, <span class="number">0.020</span>, <span class="number">0.142</span>, <span class="number">-0.638</span>, <span class="number">-0.633</span>, <span class="number">0.344</span>, <span class="number">0.868</span>, <span class="number">0.913</span>, <span class="number">0.429</span>],</span><br><span class="line">        [<span class="number">0.447</span>, <span class="number">-0.810</span>, <span class="number">-0.061</span>, <span class="number">-0.495</span>, <span class="number">0.794</span>, <span class="number">-0.064</span>, <span class="number">-0.817</span>, <span class="number">-0.408</span>, <span class="number">-0.286</span>, <span class="number">0.149</span>]]</span><br><span class="line"></span><br><span class="line">getW2 = [[<span class="number">-0.868</span>, <span class="number">-0.406</span>, <span class="number">-0.288</span>, <span class="number">-0.016</span>, <span class="number">-0.560</span>, <span class="number">0.179</span>, <span class="number">0.099</span>, <span class="number">0.438</span>, <span class="number">-0.551</span>],</span><br><span class="line">        [<span class="number">-0.395</span>, <span class="number">0.890</span>, <span class="number">0.685</span>, <span class="number">-0.329</span>, <span class="number">0.218</span>, <span class="number">-0.852</span>, <span class="number">-0.919</span>, <span class="number">0.665</span>, <span class="number">0.968</span>],</span><br><span class="line">        [<span class="number">-0.128</span>, <span class="number">0.685</span>, <span class="number">-0.828</span>, <span class="number">0.709</span>, <span class="number">-0.420</span>, <span class="number">0.057</span>, <span class="number">-0.212</span>, <span class="number">0.728</span>, <span class="number">-0.690</span>],</span><br><span class="line">        [<span class="number">0.881</span>, <span class="number">0.238</span>, <span class="number">0.018</span>, <span class="number">0.622</span>, <span class="number">0.936</span>, <span class="number">-0.442</span>, <span class="number">0.936</span>, <span class="number">0.586</span>, <span class="number">-0.020</span>],</span><br><span class="line">        [<span class="number">-0.478</span>, <span class="number">0.240</span>, <span class="number">0.820</span>, <span class="number">-0.731</span>, <span class="number">0.260</span>, <span class="number">-0.989</span>, <span class="number">-0.626</span>, <span class="number">0.796</span>, <span class="number">-0.599</span>],</span><br><span class="line">        [<span class="number">0.679</span>, <span class="number">0.721</span>, <span class="number">-0.111</span>, <span class="number">0.083</span>, <span class="number">-0.738</span>, <span class="number">0.227</span>, <span class="number">0.560</span>, <span class="number">0.929</span>, <span class="number">0.017</span>],</span><br><span class="line">        [<span class="number">-0.690</span>, <span class="number">0.907</span>, <span class="number">0.464</span>, <span class="number">-0.022</span>, <span class="number">-0.005</span>, <span class="number">-0.004</span>, <span class="number">-0.425</span>, <span class="number">0.299</span>, <span class="number">0.757</span>],</span><br><span class="line">        [<span class="number">-0.054</span>, <span class="number">0.397</span>, <span class="number">-0.017</span>, <span class="number">-0.563</span>, <span class="number">-0.551</span>, <span class="number">0.465</span>, <span class="number">-0.596</span>, <span class="number">-0.413</span>, <span class="number">-0.395</span>],</span><br><span class="line">        [<span class="number">-0.838</span>, <span class="number">0.053</span>, <span class="number">-0.160</span>, <span class="number">-0.164</span>, <span class="number">-0.671</span>, <span class="number">0.140</span>, <span class="number">-0.149</span>, <span class="number">0.708</span>, <span class="number">0.425</span>],</span><br><span class="line">        [<span class="number">0.096</span>, <span class="number">-0.995</span>, <span class="number">-0.313</span>, <span class="number">0.881</span>, <span class="number">-0.402</span>, <span class="number">-0.631</span>, <span class="number">-0.660</span>, <span class="number">0.184</span>, <span class="number">0.487</span>]]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">word2vec</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.n = settings[<span class="string">'n'</span>]</span><br><span class="line">        self.lr = settings[<span class="string">'learning_rate'</span>]</span><br><span class="line">        self.epochs = settings[<span class="string">'epochs'</span>]</span><br><span class="line">        self.window = settings[<span class="string">'window_size'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_training_data</span><span class="params">(self, settings, corpus)</span>:</span></span><br><span class="line">        <span class="comment"># Find unique word counts using dictonary</span></span><br><span class="line">        word_counts = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> corpus:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> row:</span><br><span class="line">                word_counts[word] += <span class="number">1</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(word_counts)                                                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # defaultdict(&lt;class 'int'&gt;, &#123;'natural': 1, 'language': 1, 'processing': 1, 'and': 2, 'machine': 1, 'learning': 1, 'is': 1, 'fun': 1, 'exciting': 1&#125;) #</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## How many unique words in vocab? 9</span></span><br><span class="line">        self.v_count = len(word_counts.keys())</span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line">        <span class="comment"># print(self.v_count)   #</span></span><br><span class="line">        <span class="comment"># 9                     #</span></span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate Lookup Dictionaries (vocab)</span></span><br><span class="line">        self.words_list = list(word_counts.keys())</span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.words_list)                                                                        #</span></span><br><span class="line">        <span class="comment"># ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting']  #</span></span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate word:index</span></span><br><span class="line">        self.word_index = dict((word, i) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.word_index)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # &#123;'natural': 0, 'language': 1, 'processing': 2, 'and': 3, 'machine': 4, 'learning': 5, 'is': 6, 'fun': 7, 'exciting': 8&#125; #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate index:word</span></span><br><span class="line">        self.index_word = dict((i, word) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.index_word)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># &#123;0: 'natural', 1: 'language', 2: 'processing', 3: 'and', 4: 'machine', 5: 'learning', 6: 'is', 7: 'fun', 8: 'exciting'&#125;   #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        training_data = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cycle through each sentence in corpus</span></span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">            sent_len = len(sentence)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cycle through each word in sentence</span></span><br><span class="line">            <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">                <span class="comment"># Convert target word to one-hot</span></span><br><span class="line">                w_target = self.word2onehot(sentence[i])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cycle through context window</span></span><br><span class="line">                w_context = []</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Note: window_size 2 will have range of 5 values</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i - self.window, i + self.window+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># Criteria for context word </span></span><br><span class="line">                    <span class="comment"># 1. Target word cannot be context word (j != i)</span></span><br><span class="line">                    <span class="comment"># 2. Index must be greater or equal than 0 (j &gt;= 0) - if not list index out of range</span></span><br><span class="line">                    <span class="comment"># 3. Index must be less or equal than length of sentence (j &lt;= sent_len-1) - if not list index out of range </span></span><br><span class="line">                    <span class="keyword">if</span> j != i <span class="keyword">and</span> j &lt;= sent_len<span class="number">-1</span> <span class="keyword">and</span> j &gt;= <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># Append the one-hot representation of word to w_context</span></span><br><span class="line">                        w_context.append(self.word2onehot(sentence[j]))</span><br><span class="line">                        <span class="comment"># print(sentence[i], sentence[j]) </span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        <span class="comment"># Example:              #</span></span><br><span class="line">                        <span class="comment"># natural language      #</span></span><br><span class="line">                        <span class="comment"># natural processing    #</span></span><br><span class="line">                        <span class="comment"># language natural      #</span></span><br><span class="line">                        <span class="comment"># language processing   #</span></span><br><span class="line">                        <span class="comment"># language append       #</span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># training_data contains a one-hot representation of the target word and context words</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                <span class="comment"># Example:                                                                                      #</span></span><br><span class="line">                <span class="comment"># [Target] natural, [Context] language, [Context] processing                                    #</span></span><br><span class="line">                <span class="comment"># print(training_data)                                                                          #</span></span><br><span class="line">                <span class="comment"># [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]]]   #</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                training_data.append([w_target, w_context])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(training_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2onehot</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="comment"># word_vec - initialise a blank vector</span></span><br><span class="line">        word_vec = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.v_count)] <span class="comment"># Alternative - np.zeros(self.v_count)</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line">        <span class="comment"># print(word_vec)           #</span></span><br><span class="line">        <span class="comment"># [0, 0, 0, 0, 0, 0, 0, 0]  #</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get ID of word from word_index</span></span><br><span class="line">        word_index = self.word_index[word]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Change value from 0 to 1 according to ID of the word</span></span><br><span class="line">        word_vec[word_index] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_data)</span>:</span></span><br><span class="line">        <span class="comment"># Initialising weight matrices</span></span><br><span class="line">        <span class="comment"># np.random.uniform(HIGH, LOW, OUTPUT_SHAPE)</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.uniform.html</span></span><br><span class="line">        self.w1 = np.array(getW1)</span><br><span class="line">        self.w2 = np.array(getW2)</span><br><span class="line">        <span class="comment"># self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))</span></span><br><span class="line">        <span class="comment"># self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cycle through each epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.epochs):</span><br><span class="line">            <span class="comment"># Intialise loss to 0</span></span><br><span class="line">            self.loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># Cycle through each training sample</span></span><br><span class="line">            <span class="comment"># w_t = vector for target word, w_c = vectors for context words</span></span><br><span class="line">            <span class="keyword">for</span> w_t, w_c <span class="keyword">in</span> training_data:</span><br><span class="line">                <span class="comment"># Forward pass</span></span><br><span class="line">                <span class="comment"># 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)</span></span><br><span class="line">                y_pred, h, u = self.forward_pass(w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment"># print("Vector for target word:", w_t) #</span></span><br><span class="line">                <span class="comment"># print("W1-before backprop", self.w1)  #</span></span><br><span class="line">                <span class="comment"># print("W2-before backprop", self.w2)  #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate error</span></span><br><span class="line">                <span class="comment"># 1. For a target word, calculate difference between y_pred and each of the context words</span></span><br><span class="line">                <span class="comment"># 2. Sum up the differences using np.sum to give us the error for this particular target word</span></span><br><span class="line">                EI = np.sum([np.subtract(y_pred, word) <span class="keyword">for</span> word <span class="keyword">in</span> w_c], axis=<span class="number">0</span>)</span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line">                <span class="comment"># print("Error", EI)    #</span></span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Backpropagation</span></span><br><span class="line">                <span class="comment"># We use SGD to backpropagate errors - calculate loss on the output layer </span></span><br><span class="line">                self.backprop(EI, h, w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment">#print("W1-after backprop", self.w1)    #</span></span><br><span class="line">                <span class="comment">#print("W2-after backprop", self.w2)    #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate loss</span></span><br><span class="line">                <span class="comment"># There are 2 parts to the loss function</span></span><br><span class="line">                <span class="comment"># Part 1: -ve sum of all the output +</span></span><br><span class="line">                <span class="comment"># Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)</span></span><br><span class="line">                <span class="comment"># Note: word.index(1) returns the index in the context word vector with value 1</span></span><br><span class="line">                <span class="comment"># Note: u[word.index(1)] returns the value of the output layer before softmax</span></span><br><span class="line">                self.loss += -np.sum([u[word.index(<span class="number">1</span>)] <span class="keyword">for</span> word <span class="keyword">in</span> w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">                <span class="comment"># Break if you want to see weights after first target word  #</span></span><br><span class="line">                <span class="comment"># break                                                     #</span></span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">            print(<span class="string">'Epoch:'</span>, i, <span class="string">"Loss:"</span>, self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_pass</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x is one-hot vector for target word, shape - 9x1</span></span><br><span class="line">        <span class="comment"># Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1</span></span><br><span class="line">        h = np.dot(x, self.w1)</span><br><span class="line">        <span class="comment"># Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1</span></span><br><span class="line">        u = np.dot(h, self.w2)</span><br><span class="line">        <span class="comment"># Run 1x9 through softmax to force each element to range of [0, 1] - 1x8</span></span><br><span class="line">        y_c = self.softmax(u)</span><br><span class="line">        <span class="keyword">return</span> y_c, h, u</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        e_x = np.exp(x - np.max(x))</span><br><span class="line">        <span class="keyword">return</span> e_x / e_x.sum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, e, h, x)</span>:</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html</span></span><br><span class="line">        <span class="comment"># Column vector EI represents row-wise sum of prediction errors across each context word for the current center word</span></span><br><span class="line">        <span class="comment"># Going backwards, we need to take derivative of E with respect of w2</span></span><br><span class="line">        <span class="comment"># h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9</span></span><br><span class="line">        <span class="comment"># x - shape 9x1, w2 - 10x9, e.T - 9x1</span></span><br><span class="line">        dl_dw2 = np.outer(h, e)</span><br><span class="line">        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))</span><br><span class="line">        <span class="comment">########################################</span></span><br><span class="line">        <span class="comment"># print('Delta for w2', dl_dw2)         #</span></span><br><span class="line">        <span class="comment"># print('Hidden layer', h)              #</span></span><br><span class="line">        <span class="comment"># print('np.dot', np.dot(self.w2, e.T)) #</span></span><br><span class="line">        <span class="comment"># print('Delta for w1', dl_dw1)         #</span></span><br><span class="line">        <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update weights</span></span><br><span class="line">        self.w1 = self.w1 - (self.lr * dl_dw1)</span><br><span class="line">        self.w2 = self.w2 - (self.lr * dl_dw2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get vector from word</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_vec</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        w_index = self.word_index[word]</span><br><span class="line">        v_w = self.w1[w_index]</span><br><span class="line">        <span class="keyword">return</span> v_w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input vector, returns nearest word(s)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vec_sim</span><span class="params">(self, word, top_n)</span>:</span></span><br><span class="line">        v_w1 = self.word_vec(word)</span><br><span class="line">        word_sim = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.v_count):</span><br><span class="line">            <span class="comment"># Find the similary score for each word in vocab</span></span><br><span class="line">            v_w2 = self.w1[i]</span><br><span class="line">            theta_sum = np.dot(v_w1, v_w2)</span><br><span class="line">            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)</span><br><span class="line">            theta = theta_sum / theta_den</span><br><span class="line"></span><br><span class="line">            word = self.index_word[i]</span><br><span class="line">            word_sim[word] = theta</span><br><span class="line"></span><br><span class="line">        words_sorted = sorted(word_sim.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> word, sim <span class="keyword">in</span> words_sorted[:top_n]:</span><br><span class="line">            print(word, sim)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################################</span></span><br><span class="line">settings = &#123;</span><br><span class="line">    <span class="string">'window_size'</span>: <span class="number">2</span>,           <span class="comment"># context window +- center word</span></span><br><span class="line">    <span class="string">'n'</span>: <span class="number">10</span>,                    <span class="comment"># dimensions of word embeddings, also refer to size of hidden layer</span></span><br><span class="line">    <span class="string">'epochs'</span>: <span class="number">50</span>,               <span class="comment"># number of training epochs</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.01</span>       <span class="comment"># learning rate</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text = <span class="string">"natural language processing and machine learning is fun and exciting"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note the .lower() as upper and lowercase does not matter in our implementation</span></span><br><span class="line"><span class="comment"># [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]</span></span><br><span class="line">corpus = [[word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text.split()]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialise object</span></span><br><span class="line">w2v = word2vec()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numpy ndarray with one-hot representation for [target_word, context_words]</span></span><br><span class="line">training_data = w2v.generate_training_data(settings, corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">w2v.train(training_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get vector for word</span></span><br><span class="line">word = <span class="string">"machine"</span></span><br><span class="line">vec = w2v.word_vec(word)</span><br><span class="line">print(word, vec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find similar words</span></span><br><span class="line">w2v.vec_sim(<span class="string">"machine"</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>


        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/13/word2vec原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/13/word2vec原理/" class="post-title-link" itemprop="url">word2vec原理</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-13 10:16:19" itemprop="dateCreated datePublished" datetime="2019-12-13T10:16:19+08:00">2019-12-13</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-17 17:43:26" itemprop="dateModified" datetime="2019-12-17T17:43:26+08:00">2019-12-17</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>推荐阅读：<br>Jay Alammar图解word2vec: <a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-word2vec/</a><br>word2vec——numpy原生实现: <a href="https://www.jianshu.com/p/b65f884c7237" target="_blank" rel="noopener">https://www.jianshu.com/p/b65f884c7237</a></p>
<h3 id="word2vec是什么？"><a href="#word2vec是什么？" class="headerlink" title="word2vec是什么？"></a>word2vec是什么？</h3><p>Word2vec是用来生成word embeddings的模型，将输入的语料映射到向量空间。这样做有两个好处：一是将文本用代数向量表示，机器能够理解，二则向量之间的相关性可以通过余弦距离计算。</p>
<h3 id="CBOW-and-Skip-grams"><a href="#CBOW-and-Skip-grams" class="headerlink" title="CBOW and Skip-grams"></a>CBOW and Skip-grams</h3><p>CBOW 和 Skip-grams 是word2vec的两种训练模式，产生不同的训练数据。CBOW是用上下文来预测当前词，Skip-grams是用当前词来预测上下文。<br>举个例子，假设输入语料是一句诗“生如夏花之绚烂，死如秋叶之静美。”，采用CBOW和Skip-grams两种方法获得训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"生如夏花之绚烂，死如秋叶之静美。"</span></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line">text_tokenize = nltk.word_tokenize(text)·</span><br><span class="line"><span class="comment"># text_tokenize = ["生","如","夏花","之","绚烂","，","死","如","秋叶","之","静美","。"]</span></span><br></pre></td></tr></table></figure>

<p><strong>CBOW</strong><br>CBOW将产生12个训练样本：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/cbow.png?x-oss-process=style/resize_w_1200" alt="cbow"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/cbow-data.png?x-oss-process=style/resize_w_1200" alt="cbow-data"></p>
<p><strong>Skip-grams</strong><br>Skip-grams将产生42个训练样本：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/skip-grams.png?x-oss-process=style/resize_w_1200" alt="skip-grams"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/skip-grams-data.png?x-oss-process=style/resize_w_1200" alt="skip-grams-data"></p>
<h3 id="负例采样"><a href="#负例采样" class="headerlink" title="负例采样"></a>负例采样</h3><ol>
<li>将预测相邻单词的任务，切换为提取输入与输出单词的模型，并输出一个表明它们是否是邻居的分数，0表示“不是邻居”，1表示“邻居”。这个简单的转变将神经网络改为逻辑回归模型——<strong>加快训练速度</strong>。</li>
<li>添加负样本，输出词从词汇表中随机抽取单词。<br>切换之后训练数据集的结构如下:<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sgns.png?x-oss-process=style/resize_w_1200" alt="sgns"><h3 id="Word2vec训练"><a href="#Word2vec训练" class="headerlink" title="Word2vec训练"></a>Word2vec训练</h3>word2vec训练的的两个核心思想分别是Skip-grams和负例采样。<br>训练开始时，创建两个矩阵——Embedding矩阵和Context矩阵（随机值初始化）。其中，vocab_size表示词典大小，embedding_size表示嵌入长度。<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/word2vec_weights.png?x-oss-process=style/resize_w_1200" alt="word2vec_weights"></li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/05/使用Fairseq训练gec模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/05/使用Fairseq训练gec模型/" class="post-title-link" itemprop="url">使用Fairseq训练gec模型</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-05 16:35:29" itemprop="dateCreated datePublished" datetime="2019-12-05T16:35:29+08:00">2019-12-05</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-25 18:29:40" itemprop="dateModified" datetime="2019-12-25T18:29:40+08:00">2019-12-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/thoughts/" itemprop="url" rel="index"><span itemprop="name">thoughts</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>实现过程参考猿辅导团队和Kakao团队的项目：<br>猿辅导:<a href="https://github.com/zhawe01/fairseq-gec" target="_blank" rel="noopener">https://github.com/zhawe01/fairseq-gec</a><br>Kakao:<a href="https://github.com/kakaobrain/helo_word" target="_blank" rel="noopener">https://github.com/kakaobrain/helo_word</a></p>
<h3 id="Fairseq是什么？"><a href="#Fairseq是什么？" class="headerlink" title="Fairseq是什么？"></a>Fairseq是什么？</h3><p>Fairseq(-py)是Facebook人工智能研究院研发的一个序列建模工具包，使研发人员可以训练自定义模型来进行翻译、摘要、语言建模和其他文本生成任务。它提供了各种序列到序列模型的参考实现，包括长短期记忆（LSTM）网络和新型卷积神经网络（CNN），其生成翻译的速度比相当的递归神经网络（RNN）模型快很多倍。</p>
<h3 id="fairseq-preprocess"><a href="#fairseq-preprocess" class="headerlink" title="fairseq-preprocess"></a>fairseq-preprocess</h3><p>数据预处理：建立词表、训练数据二值化</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEXT=&lt;text_path&gt;</span><br><span class="line">fairseq-preprocess --source-lang src --target-lang tgt \</span><br><span class="line">    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \</span><br><span class="line">    --destdir data-bin</span><br></pre></td></tr></table></figure>

<p><strong>Named Arguments</strong></p>
<table>
<thead>
<tr>
<th>argument</th>
<th>default</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>source-lang</td>
<td></td>
<td>源语言</td>
</tr>
<tr>
<td>target-lang</td>
<td></td>
<td>目标语言</td>
</tr>
<tr>
<td>trainpref</td>
<td></td>
<td>训练数据</td>
</tr>
<tr>
<td>validpref</td>
<td></td>
<td>验证数据</td>
</tr>
<tr>
<td>testpref</td>
<td></td>
<td>测试数据</td>
</tr>
<tr>
<td>destdir</td>
<td>data-bin</td>
<td>输出数据存放位置</td>
</tr>
<tr>
<td>output-format</td>
<td>binary, raw</td>
<td>输出数据的格式</td>
</tr>
<tr>
<td>src-dict</td>
<td></td>
<td>源语言字典</td>
</tr>
<tr>
<td>tgt-dict</td>
<td></td>
<td>目标语言字典</td>
</tr>
<tr>
<td>joined-dictionary</td>
<td></td>
<td>联合字典，复制源语言字典为目标语言字典</td>
</tr>
<tr>
<td>padding-factor</td>
<td>8</td>
<td>填充字典大小为N的倍数</td>
</tr>
<tr>
<td>task</td>
<td>translation</td>
<td>任务类型</td>
</tr>
</tbody></table>
<p><strong>举个例子</strong><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/fairseq-preprocess.png?x-oss-process=style/resize_w_1200" alt="fairseq-preprocess"><br>生成的词表、训练数据如下图所示:<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/fairseq_1.png?x-oss-process=style/resize_w_1200" alt="fairseq_1"></p>
<h3 id="fairseq-train"><a href="#fairseq-train" class="headerlink" title="fairseq-train"></a>fairseq-train</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en \</span><br><span class="line">    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \</span><br><span class="line">    --arch fconv_iwslt_de_en --save-dir checkpoints/fconv</span><br></pre></td></tr></table></figure>

<p>fairseq-train 默认使用机器上全部的gpu, 使用CUDA_VISIBLE_DEVICES可以指定gpu<br><strong>Named Arguments</strong></p>
<table>
<thead>
<tr>
<th>argument</th>
<th>default</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>data-bin</td>
<td></td>
<td>预处理之后的数据</td>
</tr>
<tr>
<td>arch</td>
<td>fconv</td>
<td>模型架构</td>
</tr>
<tr>
<td>save-dir</td>
<td></td>
<td>模型存储位置</td>
</tr>
<tr>
<td>max-epoch</td>
<td>0</td>
<td>停止训练的epoch</td>
</tr>
<tr>
<td>max-tokens</td>
<td>4000</td>
<td>一个batch中最多的token数量</td>
</tr>
<tr>
<td>num-workers</td>
<td>1</td>
<td>数据加载的子进程个数</td>
</tr>
<tr>
<td>pretrained-model</td>
<td></td>
<td>预训练模型的位置</td>
</tr>
<tr>
<td>train-subset</td>
<td>train</td>
<td>训练数据集</td>
</tr>
<tr>
<td>valid-subset</td>
<td>valid</td>
<td>验证数据集</td>
</tr>
<tr>
<td>lazy-load</td>
<td></td>
<td>数据懒加载</td>
</tr>
</tbody></table>
<p><strong>举个例子</strong></p>
<h3 id="fairseq-generate"><a href="#fairseq-generate" class="headerlink" title="fairseq-generate"></a>fairseq-generate</h3><p>用训练好的模型翻译预处理后的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DATA_BIN=&lt;data_bin_path&gt;</span><br><span class="line">MODEL_DIR=&lt;model_path&gt;</span><br><span class="line">fairseq-generate $DATA_BIN \</span><br><span class="line">    --path $MODEL_DIR/model.pt \</span><br><span class="line">    --batch-size 128 --beam 5</span><br></pre></td></tr></table></figure>

<h3 id="fairseq-interactive"><a href="#fairseq-interactive" class="headerlink" title="fairseq-interactive"></a>fairseq-interactive</h3><p>用训练好的模型翻译原始数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR=&lt;model_path&gt;</span><br><span class="line">fairseq-interactive \</span><br><span class="line">    --path $MODEL_DIR/model.pt $MODEL_DIR \</span><br><span class="line">    --beam 5 --source-lang src --target-lang tgt \</span><br><span class="line">    --tokenizer moses \</span><br><span class="line">    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes</span><br></pre></td></tr></table></figure>

<p><strong>raw-text</strong>指的是分词后的句子，<strong>pre-processed data</strong>指的是bin文件</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/04/Errant在GEC中的应用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/04/Errant在GEC中的应用/" class="post-title-link" itemprop="url">Errant在GEC中的应用</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-04 10:21:43 / Modified: 18:59:54" itemprop="dateCreated datePublished" datetime="2019-12-04T10:21:43+08:00">2019-12-04</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/thoughts/" itemprop="url" rel="index"><span itemprop="name">thoughts</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>errant: <a href="https://github.com/chrisjbryant/errant" target="_blank" rel="noopener">https://github.com/chrisjbryant/errant</a><br>Kakao: <a href="https://github.com/kakaobrain/helo_word" target="_blank" rel="noopener">https://github.com/kakaobrain/helo_word</a></p>
<h3 id="Errant提供的方法"><a href="#Errant提供的方法" class="headerlink" title="Errant提供的方法"></a>Errant提供的方法</h3><p>Errant是一个语法错误标注工具，可以从平行语料中自动提取edits，并进行错误分类。Errant工具提供以下几个方法：</p>
<ol>
<li><code>parallel_to_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 parallel_to_m2.py -orig &lt;orig_file&gt; -cor &lt;cor_file1&gt; [&lt;cor_file2&gt; ...] -out &lt;out_m2&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>从平行语料自动提取edits并进行分类，运行该命令需要一个original文件，及至少一个corrected文件，指定一个输出M2文件名。<strong>original和corrected文件都必须一行一个句子，并且需要分词</strong><br><strong>举个例子</strong><br><strong>Original</strong>: This are gramamtical sentence .<br><strong>Corrected</strong>: This is a grammatical sentence .<br><strong>Output M2</strong>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">S This are gramamtical sentence .  </span><br><span class="line">A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>m2_to_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 m2_to_m2.py &#123;-auto|-gold&#125; m2_file -out &lt;out_m2&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>将M2文件转成M2文件。用处在于，有一些M2文件是由专家标注，错误分类和标注方法与Errant不同，此方法可以将这类型的M2转换到Errant格式上，统一错误类型。<br><code>-gold</code>只将现存的edits进行分类<br><code>-auto</code>自动提取edits并且分类<br><strong>conll2014.m2是一个由专家标注的包含1312对平行语料的公开gec任务测试集，使用m2_to_m2的方法，将conll2014.m2转换成conll2014.errant.m2之后，不影响评价结果</strong></p>
<ol start="3">
<li><code>compare_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; </span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -cat &#123;1,2,3&#125;</span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -ds</span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -ds -cat &#123;1,2,3&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>比较M2文件，可以用来评估猜想的M2与参考的M2的相似程度。评估结果给出precision、recall、F-score (F 0.5)</p>
<h3 id="Errant在GEC中的应用"><a href="#Errant在GEC中的应用" class="headerlink" title="Errant在GEC中的应用"></a>Errant在GEC中的应用</h3><p>Kakao团队在2019 BEA GEC共享任务中，取得领先成果。<br>参考Kakao团队的方法，在gec中应用errant，对gec系统进行效果评估，控制输出的错误类型，提升精度。工作流程见下图：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/errant_gec.png?x-oss-process=style/resize_w_1200" alt="gec_errant"><br>图中<strong>conll2014.ori</strong>与<strong>conll2014.gold.m2</strong>为公开测试集的original数据与专家标注的m2文件。流程解读：</p>
<ol>
<li>将<strong>conll2014.ori</strong>数据输入gec系统中，得到预测结果<strong>conll2014.pred</strong></li>
<li>将<strong>conll2014.ori</strong>与<strong>conll2014.pred</strong>作为平行语料，使用<code>parallel_to_m2.py</code>方法，得到<strong>conll2014.pred._m2</strong></li>
<li>使用<code>compare_m2.py</code>比较两个m2文件: <strong>conll2014.pred._m2</strong>和<strong>conll2014.gold.m2</strong>，得到<strong>conll2014.report</strong></li>
<li>使用<code>error_type_control.py</code>分析<strong>conll2014.report</strong>,找到精度低的错误类型</li>
<li>除去精度低的错误类型，再输入gec系统中，得到纠正的结果<strong>conll2014.cor</strong></li>
</ol>
<p><code>error_type_control.py</code>是Kakao团队提供的方法，输入report，自动找出精度低的错误类型的组合，输出一个错误类型列表。Errant根据规则制定了25种错误类型，有一些类型并不常见，召回率低，有一些类型常见，但是精度低。通过<code>error_type_control.py</code>过滤掉一些不常见、精度低的错误类型，可以提升整体的表现。</p>
<p><strong>conll2014.report长这样</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">===================== Span-Based Correction ======================</span><br><span class="line">Category       TP       FP       FN       P        R        F0<span class="number">.5</span></span><br><span class="line">M:ADJ          <span class="number">0</span>        <span class="number">1</span>        <span class="number">3</span>        <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">M:ADV          <span class="number">2</span>        <span class="number">1</span>        <span class="number">4</span>        <span class="number">0.6667</span>   <span class="number">0.3333</span>   <span class="number">0.5556</span></span><br><span class="line">M:CONJ         <span class="number">0</span>        <span class="number">1</span>        <span class="number">6</span>        <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">M:DET          <span class="number">73</span>       <span class="number">44</span>       <span class="number">65</span>       <span class="number">0.6239</span>   <span class="number">0.529</span>    <span class="number">0.6023</span></span><br><span class="line">M:NOUN         <span class="number">1</span>        <span class="number">7</span>        <span class="number">13</span>       <span class="number">0.125</span>    <span class="number">0.0714</span>   <span class="number">0.1087</span></span><br><span class="line">M:NOUN:POSS    <span class="number">3</span>        <span class="number">2</span>        <span class="number">5</span>        <span class="number">0.6</span>      <span class="number">0.375</span>    <span class="number">0.5357</span></span><br><span class="line">M:OTHER        <span class="number">2</span>        <span class="number">4</span>        <span class="number">19</span>       <span class="number">0.3333</span>   <span class="number">0.0952</span>   <span class="number">0.2222</span></span><br><span class="line">M:PART         <span class="number">2</span>        <span class="number">3</span>        <span class="number">6</span>        <span class="number">0.4</span>      <span class="number">0.25</span>     <span class="number">0.3571</span></span><br><span class="line">M:PREP         <span class="number">28</span>       <span class="number">22</span>       <span class="number">30</span>       <span class="number">0.56</span>     <span class="number">0.4828</span>   <span class="number">0.5426</span></span><br><span class="line">M:PRON         <span class="number">2</span>        <span class="number">9</span>        <span class="number">10</span>       <span class="number">0.1818</span>   <span class="number">0.1667</span>   <span class="number">0.1786</span></span><br><span class="line">M:PUNCT        <span class="number">43</span>       <span class="number">30</span>       <span class="number">75</span>       <span class="number">0.589</span>    <span class="number">0.3644</span>   <span class="number">0.5244</span></span><br><span class="line">M:VERB         <span class="number">7</span>        <span class="number">11</span>       <span class="number">14</span>       <span class="number">0.3889</span>   <span class="number">0.3333</span>   <span class="number">0.3763</span></span><br><span class="line">M:VERB:FORM    <span class="number">2</span>        <span class="number">1</span>        <span class="number">4</span>        <span class="number">0.6667</span>   <span class="number">0.3333</span>   <span class="number">0.5556</span></span><br><span class="line">M:VERB:TENSE   <span class="number">10</span>       <span class="number">9</span>        <span class="number">14</span>       <span class="number">0.5263</span>   <span class="number">0.4167</span>   <span class="number">0.5</span></span><br><span class="line">R:ADJ          <span class="number">1</span>        <span class="number">11</span>       <span class="number">20</span>       <span class="number">0.0833</span>   <span class="number">0.0476</span>   <span class="number">0.0725</span></span><br><span class="line">R:ADJ:FORM     <span class="number">7</span>        <span class="number">0</span>        <span class="number">4</span>        <span class="number">1.0</span>      <span class="number">0.6364</span>   <span class="number">0.8974</span></span><br><span class="line">R:ADV          <span class="number">2</span>        <span class="number">2</span>        <span class="number">11</span>       <span class="number">0.5</span>      <span class="number">0.1538</span>   <span class="number">0.3448</span></span><br><span class="line">R:CONJ         <span class="number">0</span>        <span class="number">0</span>        <span class="number">6</span>        <span class="number">1.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">R:CONTR        <span class="number">0</span>        <span class="number">0</span>        <span class="number">1</span>        <span class="number">1.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">R:DET          <span class="number">31</span>       <span class="number">39</span>       <span class="number">42</span>       <span class="number">0.4429</span>   <span class="number">0.4247</span>   <span class="number">0.4391</span></span><br><span class="line">R:MORPH        <span class="number">38</span>       <span class="number">15</span>       <span class="number">38</span>       <span class="number">0.717</span>    <span class="number">0.5</span>      <span class="number">0.6597</span></span><br><span class="line">R:NOUN         <span class="number">13</span>       <span class="number">36</span>       <span class="number">68</span>       <span class="number">0.2653</span>   <span class="number">0.1605</span>   <span class="number">0.2347</span></span><br><span class="line">R:NOUN:INFL    <span class="number">5</span>        <span class="number">4</span>        <span class="number">1</span>        <span class="number">0.5556</span>   <span class="number">0.8333</span>   <span class="number">0.5952</span></span><br><span class="line">R:NOUN:NUM     <span class="number">156</span>      <span class="number">66</span>       <span class="number">71</span>       <span class="number">0.7027</span>   <span class="number">0.6872</span>   <span class="number">0.6996</span></span><br><span class="line">R:NOUN:POSS    <span class="number">3</span>        <span class="number">1</span>        <span class="number">7</span>        <span class="number">0.75</span>     <span class="number">0.3</span>      <span class="number">0.5769</span></span><br><span class="line">R:ORTH         <span class="number">32</span>       <span class="number">14</span>       <span class="number">18</span>       <span class="number">0.6957</span>   <span class="number">0.64</span>     <span class="number">0.6838</span></span><br><span class="line">R:OTHER        <span class="number">28</span>       <span class="number">93</span>       <span class="number">268</span>      <span class="number">0.2314</span>   <span class="number">0.0946</span>   <span class="number">0.1795</span></span><br><span class="line">R:PART         <span class="number">17</span>       <span class="number">10</span>       <span class="number">8</span>        <span class="number">0.6296</span>   <span class="number">0.68</span>     <span class="number">0.6391</span></span><br><span class="line">R:PREP         <span class="number">72</span>       <span class="number">56</span>       <span class="number">68</span>       <span class="number">0.5625</span>   <span class="number">0.5143</span>   <span class="number">0.5521</span></span><br><span class="line">R:PRON         <span class="number">12</span>       <span class="number">10</span>       <span class="number">22</span>       <span class="number">0.5455</span>   <span class="number">0.3529</span>   <span class="number">0.4918</span></span><br><span class="line">R:PUNCT        <span class="number">16</span>       <span class="number">13</span>       <span class="number">31</span>       <span class="number">0.5517</span>   <span class="number">0.3404</span>   <span class="number">0.4908</span></span><br><span class="line">R:SPELL        <span class="number">81</span>       <span class="number">24</span>       <span class="number">17</span>       <span class="number">0.7714</span>   <span class="number">0.8265</span>   <span class="number">0.7819</span></span><br><span class="line">R:VERB         <span class="number">17</span>       <span class="number">18</span>       <span class="number">98</span>       <span class="number">0.4857</span>   <span class="number">0.1478</span>   <span class="number">0.3333</span></span><br><span class="line">R:VERB:FORM    <span class="number">68</span>       <span class="number">33</span>       <span class="number">35</span>       <span class="number">0.6733</span>   <span class="number">0.6602</span>   <span class="number">0.6706</span></span><br><span class="line">R:VERB:INFL    <span class="number">1</span>        <span class="number">0</span>        <span class="number">1</span>        <span class="number">1.0</span>      <span class="number">0.5</span>      <span class="number">0.8333</span></span><br><span class="line">R:VERB:SVA     <span class="number">76</span>       <span class="number">29</span>       <span class="number">37</span>       <span class="number">0.7238</span>   <span class="number">0.6726</span>   <span class="number">0.7129</span></span><br><span class="line">R:VERB:TENSE   <span class="number">32</span>       <span class="number">27</span>       <span class="number">98</span>       <span class="number">0.5424</span>   <span class="number">0.2462</span>   <span class="number">0.4372</span></span><br><span class="line">R:WO           <span class="number">8</span>        <span class="number">6</span>        <span class="number">11</span>       <span class="number">0.5714</span>   <span class="number">0.4211</span>   <span class="number">0.5333</span></span><br><span class="line">U:ADJ          <span class="number">0</span>        <span class="number">3</span>        <span class="number">10</span>       <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">U:ADV          <span class="number">3</span>        <span class="number">3</span>        <span class="number">12</span>       <span class="number">0.5</span>      <span class="number">0.2</span>      <span class="number">0.3846</span></span><br><span class="line">U:CONJ         <span class="number">1</span>        <span class="number">0</span>        <span class="number">3</span>        <span class="number">1.0</span>      <span class="number">0.25</span>     <span class="number">0.625</span></span><br><span class="line">U:DET          <span class="number">115</span>      <span class="number">77</span>       <span class="number">72</span>       <span class="number">0.599</span>    <span class="number">0.615</span>    <span class="number">0.6021</span></span><br><span class="line">U:NOUN         <span class="number">1</span>        <span class="number">2</span>        <span class="number">18</span>       <span class="number">0.3333</span>   <span class="number">0.0526</span>   <span class="number">0.1613</span></span><br><span class="line">U:NOUN:POSS    <span class="number">1</span>        <span class="number">0</span>        <span class="number">3</span>        <span class="number">1.0</span>      <span class="number">0.25</span>     <span class="number">0.625</span></span><br><span class="line">U:OTHER        <span class="number">3</span>        <span class="number">20</span>       <span class="number">47</span>       <span class="number">0.1304</span>   <span class="number">0.06</span>     <span class="number">0.1056</span></span><br><span class="line">U:PART         <span class="number">2</span>        <span class="number">1</span>        <span class="number">5</span>        <span class="number">0.6667</span>   <span class="number">0.2857</span>   <span class="number">0.5263</span></span><br><span class="line">U:PREP         <span class="number">43</span>       <span class="number">15</span>       <span class="number">29</span>       <span class="number">0.7414</span>   <span class="number">0.5972</span>   <span class="number">0.7072</span></span><br><span class="line">U:PRON         <span class="number">1</span>        <span class="number">2</span>        <span class="number">10</span>       <span class="number">0.3333</span>   <span class="number">0.0909</span>   <span class="number">0.2174</span></span><br><span class="line">U:PUNCT        <span class="number">5</span>        <span class="number">3</span>        <span class="number">10</span>       <span class="number">0.625</span>    <span class="number">0.3333</span>   <span class="number">0.5319</span></span><br><span class="line">U:VERB         <span class="number">8</span>        <span class="number">5</span>        <span class="number">18</span>       <span class="number">0.6154</span>   <span class="number">0.3077</span>   <span class="number">0.5128</span></span><br><span class="line">U:VERB:FORM    <span class="number">4</span>        <span class="number">0</span>        <span class="number">2</span>        <span class="number">1.0</span>      <span class="number">0.6667</span>   <span class="number">0.9091</span></span><br><span class="line">U:VERB:TENSE   <span class="number">12</span>       <span class="number">8</span>        <span class="number">14</span>       <span class="number">0.6</span>      <span class="number">0.4615</span>   <span class="number">0.566</span></span><br><span class="line"></span><br><span class="line">=========== Span-Based Correction ============</span><br><span class="line">TP	FP	FN	Prec	Rec	F0<span class="number">.5</span></span><br><span class="line"><span class="number">1090</span>	<span class="number">791</span>	<span class="number">1502</span>	<span class="number">0.5795</span>	<span class="number">0.4205</span>	<span class="number">0.5388</span></span><br><span class="line">==============================================</span><br></pre></td></tr></table></figure>

<p><strong>在我的项目中，主要注重提升精度，过滤错误类型之后，在conll2014测试集上精度提升3到4个百分点</strong></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="LNdoremi">
  <p class="site-author-name" itemprop="name">LNdoremi</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/LNdoremi" title="GitHub &rarr; https://github.com/LNdoremi" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/liuna_sysu@qq.com" title="E-Mail &rarr; liuna_sysu@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.archao.me/" title="https://www.archao.me/" rel="noopener" target="_blank">Chao</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LNdoremi</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  

  

</body>
</html>
