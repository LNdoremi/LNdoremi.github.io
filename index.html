<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LNdoremi">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="LNdoremi">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LNdoremi">
  <link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>LNdoremi</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LNdoremi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/23/pytorch-tensor-常用运算/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/23/pytorch-tensor-常用运算/" class="post-title-link" itemprop="url">pytorch-tensor-常用运算</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-23 17:48:35 / Modified: 17:50:15" itemprop="dateCreated datePublished" datetime="2019-12-23T17:48:35+08:00">2019-12-23</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="pytorch-tensor-常用运算"><a href="#pytorch-tensor-常用运算" class="headerlink" title="pytorch-tensor-常用运算"></a>pytorch-tensor-常用运算</h3>
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/17/word2vec的实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/17/word2vec的实现/" class="post-title-link" itemprop="url">word2vec的实现</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-17 18:23:26 / Modified: 18:24:44" itemprop="dateCreated datePublished" datetime="2019-12-17T18:23:26+08:00">2019-12-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This is a implementation of Word2Vec using numpy. Uncomment the print functions to see Word2Vec in action! Also remember to change the number of epochs and set training_data to training_data[0] to avoid flooding your terminal. A Google Sheet implementation of Word2Vec is also available here - https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing</span></span><br><span class="line"><span class="string">Have fun learning!</span></span><br><span class="line"><span class="string">Author: Derek Chia</span></span><br><span class="line"><span class="string">Email: derek@derekchia.com</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment">## Randomly initialise</span></span><br><span class="line">getW1 = [[<span class="number">0.236</span>, <span class="number">-0.962</span>, <span class="number">0.686</span>, <span class="number">0.785</span>, <span class="number">-0.454</span>, <span class="number">-0.833</span>, <span class="number">-0.744</span>, <span class="number">0.677</span>, <span class="number">-0.427</span>, <span class="number">-0.066</span>],</span><br><span class="line">        [<span class="number">-0.907</span>, <span class="number">0.894</span>, <span class="number">0.225</span>, <span class="number">0.673</span>, <span class="number">-0.579</span>, <span class="number">-0.428</span>, <span class="number">0.685</span>, <span class="number">0.973</span>, <span class="number">-0.070</span>, <span class="number">-0.811</span>],</span><br><span class="line">        [<span class="number">-0.576</span>, <span class="number">0.658</span>, <span class="number">-0.582</span>, <span class="number">-0.112</span>, <span class="number">0.662</span>, <span class="number">0.051</span>, <span class="number">-0.401</span>, <span class="number">-0.921</span>, <span class="number">-0.158</span>, <span class="number">0.529</span>],</span><br><span class="line">        [<span class="number">0.517</span>, <span class="number">0.436</span>, <span class="number">0.092</span>, <span class="number">-0.835</span>, <span class="number">-0.444</span>, <span class="number">-0.905</span>, <span class="number">0.879</span>, <span class="number">0.303</span>, <span class="number">0.332</span>, <span class="number">-0.275</span>],</span><br><span class="line">        [<span class="number">0.859</span>, <span class="number">-0.890</span>, <span class="number">0.651</span>, <span class="number">0.185</span>, <span class="number">-0.511</span>, <span class="number">-0.456</span>, <span class="number">0.377</span>, <span class="number">-0.274</span>, <span class="number">0.182</span>, <span class="number">-0.237</span>],</span><br><span class="line">        [<span class="number">0.368</span>, <span class="number">-0.867</span>, <span class="number">-0.301</span>, <span class="number">-0.222</span>, <span class="number">0.630</span>, <span class="number">0.808</span>, <span class="number">0.088</span>, <span class="number">-0.902</span>, <span class="number">-0.450</span>, <span class="number">-0.408</span>],</span><br><span class="line">        [<span class="number">0.728</span>, <span class="number">0.277</span>, <span class="number">0.439</span>, <span class="number">0.138</span>, <span class="number">-0.943</span>, <span class="number">-0.409</span>, <span class="number">0.687</span>, <span class="number">-0.215</span>, <span class="number">-0.807</span>, <span class="number">0.612</span>],</span><br><span class="line">        [<span class="number">0.593</span>, <span class="number">-0.699</span>, <span class="number">0.020</span>, <span class="number">0.142</span>, <span class="number">-0.638</span>, <span class="number">-0.633</span>, <span class="number">0.344</span>, <span class="number">0.868</span>, <span class="number">0.913</span>, <span class="number">0.429</span>],</span><br><span class="line">        [<span class="number">0.447</span>, <span class="number">-0.810</span>, <span class="number">-0.061</span>, <span class="number">-0.495</span>, <span class="number">0.794</span>, <span class="number">-0.064</span>, <span class="number">-0.817</span>, <span class="number">-0.408</span>, <span class="number">-0.286</span>, <span class="number">0.149</span>]]</span><br><span class="line"></span><br><span class="line">getW2 = [[<span class="number">-0.868</span>, <span class="number">-0.406</span>, <span class="number">-0.288</span>, <span class="number">-0.016</span>, <span class="number">-0.560</span>, <span class="number">0.179</span>, <span class="number">0.099</span>, <span class="number">0.438</span>, <span class="number">-0.551</span>],</span><br><span class="line">        [<span class="number">-0.395</span>, <span class="number">0.890</span>, <span class="number">0.685</span>, <span class="number">-0.329</span>, <span class="number">0.218</span>, <span class="number">-0.852</span>, <span class="number">-0.919</span>, <span class="number">0.665</span>, <span class="number">0.968</span>],</span><br><span class="line">        [<span class="number">-0.128</span>, <span class="number">0.685</span>, <span class="number">-0.828</span>, <span class="number">0.709</span>, <span class="number">-0.420</span>, <span class="number">0.057</span>, <span class="number">-0.212</span>, <span class="number">0.728</span>, <span class="number">-0.690</span>],</span><br><span class="line">        [<span class="number">0.881</span>, <span class="number">0.238</span>, <span class="number">0.018</span>, <span class="number">0.622</span>, <span class="number">0.936</span>, <span class="number">-0.442</span>, <span class="number">0.936</span>, <span class="number">0.586</span>, <span class="number">-0.020</span>],</span><br><span class="line">        [<span class="number">-0.478</span>, <span class="number">0.240</span>, <span class="number">0.820</span>, <span class="number">-0.731</span>, <span class="number">0.260</span>, <span class="number">-0.989</span>, <span class="number">-0.626</span>, <span class="number">0.796</span>, <span class="number">-0.599</span>],</span><br><span class="line">        [<span class="number">0.679</span>, <span class="number">0.721</span>, <span class="number">-0.111</span>, <span class="number">0.083</span>, <span class="number">-0.738</span>, <span class="number">0.227</span>, <span class="number">0.560</span>, <span class="number">0.929</span>, <span class="number">0.017</span>],</span><br><span class="line">        [<span class="number">-0.690</span>, <span class="number">0.907</span>, <span class="number">0.464</span>, <span class="number">-0.022</span>, <span class="number">-0.005</span>, <span class="number">-0.004</span>, <span class="number">-0.425</span>, <span class="number">0.299</span>, <span class="number">0.757</span>],</span><br><span class="line">        [<span class="number">-0.054</span>, <span class="number">0.397</span>, <span class="number">-0.017</span>, <span class="number">-0.563</span>, <span class="number">-0.551</span>, <span class="number">0.465</span>, <span class="number">-0.596</span>, <span class="number">-0.413</span>, <span class="number">-0.395</span>],</span><br><span class="line">        [<span class="number">-0.838</span>, <span class="number">0.053</span>, <span class="number">-0.160</span>, <span class="number">-0.164</span>, <span class="number">-0.671</span>, <span class="number">0.140</span>, <span class="number">-0.149</span>, <span class="number">0.708</span>, <span class="number">0.425</span>],</span><br><span class="line">        [<span class="number">0.096</span>, <span class="number">-0.995</span>, <span class="number">-0.313</span>, <span class="number">0.881</span>, <span class="number">-0.402</span>, <span class="number">-0.631</span>, <span class="number">-0.660</span>, <span class="number">0.184</span>, <span class="number">0.487</span>]]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">word2vec</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.n = settings[<span class="string">'n'</span>]</span><br><span class="line">        self.lr = settings[<span class="string">'learning_rate'</span>]</span><br><span class="line">        self.epochs = settings[<span class="string">'epochs'</span>]</span><br><span class="line">        self.window = settings[<span class="string">'window_size'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_training_data</span><span class="params">(self, settings, corpus)</span>:</span></span><br><span class="line">        <span class="comment"># Find unique word counts using dictonary</span></span><br><span class="line">        word_counts = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> corpus:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> row:</span><br><span class="line">                word_counts[word] += <span class="number">1</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(word_counts)                                                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # defaultdict(&lt;class 'int'&gt;, &#123;'natural': 1, 'language': 1, 'processing': 1, 'and': 2, 'machine': 1, 'learning': 1, 'is': 1, 'fun': 1, 'exciting': 1&#125;) #</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## How many unique words in vocab? 9</span></span><br><span class="line">        self.v_count = len(word_counts.keys())</span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line">        <span class="comment"># print(self.v_count)   #</span></span><br><span class="line">        <span class="comment"># 9                     #</span></span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate Lookup Dictionaries (vocab)</span></span><br><span class="line">        self.words_list = list(word_counts.keys())</span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.words_list)                                                                        #</span></span><br><span class="line">        <span class="comment"># ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting']  #</span></span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate word:index</span></span><br><span class="line">        self.word_index = dict((word, i) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.word_index)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # &#123;'natural': 0, 'language': 1, 'processing': 2, 'and': 3, 'machine': 4, 'learning': 5, 'is': 6, 'fun': 7, 'exciting': 8&#125; #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate index:word</span></span><br><span class="line">        self.index_word = dict((i, word) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.index_word)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># &#123;0: 'natural', 1: 'language', 2: 'processing', 3: 'and', 4: 'machine', 5: 'learning', 6: 'is', 7: 'fun', 8: 'exciting'&#125;   #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        training_data = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cycle through each sentence in corpus</span></span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">            sent_len = len(sentence)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cycle through each word in sentence</span></span><br><span class="line">            <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">                <span class="comment"># Convert target word to one-hot</span></span><br><span class="line">                w_target = self.word2onehot(sentence[i])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cycle through context window</span></span><br><span class="line">                w_context = []</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Note: window_size 2 will have range of 5 values</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i - self.window, i + self.window+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># Criteria for context word </span></span><br><span class="line">                    <span class="comment"># 1. Target word cannot be context word (j != i)</span></span><br><span class="line">                    <span class="comment"># 2. Index must be greater or equal than 0 (j &gt;= 0) - if not list index out of range</span></span><br><span class="line">                    <span class="comment"># 3. Index must be less or equal than length of sentence (j &lt;= sent_len-1) - if not list index out of range </span></span><br><span class="line">                    <span class="keyword">if</span> j != i <span class="keyword">and</span> j &lt;= sent_len<span class="number">-1</span> <span class="keyword">and</span> j &gt;= <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># Append the one-hot representation of word to w_context</span></span><br><span class="line">                        w_context.append(self.word2onehot(sentence[j]))</span><br><span class="line">                        <span class="comment"># print(sentence[i], sentence[j]) </span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        <span class="comment"># Example:              #</span></span><br><span class="line">                        <span class="comment"># natural language      #</span></span><br><span class="line">                        <span class="comment"># natural processing    #</span></span><br><span class="line">                        <span class="comment"># language natural      #</span></span><br><span class="line">                        <span class="comment"># language processing   #</span></span><br><span class="line">                        <span class="comment"># language append       #</span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># training_data contains a one-hot representation of the target word and context words</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                <span class="comment"># Example:                                                                                      #</span></span><br><span class="line">                <span class="comment"># [Target] natural, [Context] language, [Context] processing                                    #</span></span><br><span class="line">                <span class="comment"># print(training_data)                                                                          #</span></span><br><span class="line">                <span class="comment"># [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]]]   #</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                training_data.append([w_target, w_context])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(training_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2onehot</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="comment"># word_vec - initialise a blank vector</span></span><br><span class="line">        word_vec = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.v_count)] <span class="comment"># Alternative - np.zeros(self.v_count)</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line">        <span class="comment"># print(word_vec)           #</span></span><br><span class="line">        <span class="comment"># [0, 0, 0, 0, 0, 0, 0, 0]  #</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get ID of word from word_index</span></span><br><span class="line">        word_index = self.word_index[word]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Change value from 0 to 1 according to ID of the word</span></span><br><span class="line">        word_vec[word_index] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_data)</span>:</span></span><br><span class="line">        <span class="comment"># Initialising weight matrices</span></span><br><span class="line">        <span class="comment"># np.random.uniform(HIGH, LOW, OUTPUT_SHAPE)</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.uniform.html</span></span><br><span class="line">        self.w1 = np.array(getW1)</span><br><span class="line">        self.w2 = np.array(getW2)</span><br><span class="line">        <span class="comment"># self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))</span></span><br><span class="line">        <span class="comment"># self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cycle through each epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.epochs):</span><br><span class="line">            <span class="comment"># Intialise loss to 0</span></span><br><span class="line">            self.loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># Cycle through each training sample</span></span><br><span class="line">            <span class="comment"># w_t = vector for target word, w_c = vectors for context words</span></span><br><span class="line">            <span class="keyword">for</span> w_t, w_c <span class="keyword">in</span> training_data:</span><br><span class="line">                <span class="comment"># Forward pass</span></span><br><span class="line">                <span class="comment"># 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)</span></span><br><span class="line">                y_pred, h, u = self.forward_pass(w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment"># print("Vector for target word:", w_t) #</span></span><br><span class="line">                <span class="comment"># print("W1-before backprop", self.w1)  #</span></span><br><span class="line">                <span class="comment"># print("W2-before backprop", self.w2)  #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate error</span></span><br><span class="line">                <span class="comment"># 1. For a target word, calculate difference between y_pred and each of the context words</span></span><br><span class="line">                <span class="comment"># 2. Sum up the differences using np.sum to give us the error for this particular target word</span></span><br><span class="line">                EI = np.sum([np.subtract(y_pred, word) <span class="keyword">for</span> word <span class="keyword">in</span> w_c], axis=<span class="number">0</span>)</span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line">                <span class="comment"># print("Error", EI)    #</span></span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Backpropagation</span></span><br><span class="line">                <span class="comment"># We use SGD to backpropagate errors - calculate loss on the output layer </span></span><br><span class="line">                self.backprop(EI, h, w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment">#print("W1-after backprop", self.w1)    #</span></span><br><span class="line">                <span class="comment">#print("W2-after backprop", self.w2)    #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate loss</span></span><br><span class="line">                <span class="comment"># There are 2 parts to the loss function</span></span><br><span class="line">                <span class="comment"># Part 1: -ve sum of all the output +</span></span><br><span class="line">                <span class="comment"># Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)</span></span><br><span class="line">                <span class="comment"># Note: word.index(1) returns the index in the context word vector with value 1</span></span><br><span class="line">                <span class="comment"># Note: u[word.index(1)] returns the value of the output layer before softmax</span></span><br><span class="line">                self.loss += -np.sum([u[word.index(<span class="number">1</span>)] <span class="keyword">for</span> word <span class="keyword">in</span> w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">                <span class="comment"># Break if you want to see weights after first target word  #</span></span><br><span class="line">                <span class="comment"># break                                                     #</span></span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">            print(<span class="string">'Epoch:'</span>, i, <span class="string">"Loss:"</span>, self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_pass</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x is one-hot vector for target word, shape - 9x1</span></span><br><span class="line">        <span class="comment"># Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1</span></span><br><span class="line">        h = np.dot(x, self.w1)</span><br><span class="line">        <span class="comment"># Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1</span></span><br><span class="line">        u = np.dot(h, self.w2)</span><br><span class="line">        <span class="comment"># Run 1x9 through softmax to force each element to range of [0, 1] - 1x8</span></span><br><span class="line">        y_c = self.softmax(u)</span><br><span class="line">        <span class="keyword">return</span> y_c, h, u</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        e_x = np.exp(x - np.max(x))</span><br><span class="line">        <span class="keyword">return</span> e_x / e_x.sum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, e, h, x)</span>:</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html</span></span><br><span class="line">        <span class="comment"># Column vector EI represents row-wise sum of prediction errors across each context word for the current center word</span></span><br><span class="line">        <span class="comment"># Going backwards, we need to take derivative of E with respect of w2</span></span><br><span class="line">        <span class="comment"># h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9</span></span><br><span class="line">        <span class="comment"># x - shape 9x1, w2 - 10x9, e.T - 9x1</span></span><br><span class="line">        dl_dw2 = np.outer(h, e)</span><br><span class="line">        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))</span><br><span class="line">        <span class="comment">########################################</span></span><br><span class="line">        <span class="comment"># print('Delta for w2', dl_dw2)         #</span></span><br><span class="line">        <span class="comment"># print('Hidden layer', h)              #</span></span><br><span class="line">        <span class="comment"># print('np.dot', np.dot(self.w2, e.T)) #</span></span><br><span class="line">        <span class="comment"># print('Delta for w1', dl_dw1)         #</span></span><br><span class="line">        <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update weights</span></span><br><span class="line">        self.w1 = self.w1 - (self.lr * dl_dw1)</span><br><span class="line">        self.w2 = self.w2 - (self.lr * dl_dw2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get vector from word</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_vec</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        w_index = self.word_index[word]</span><br><span class="line">        v_w = self.w1[w_index]</span><br><span class="line">        <span class="keyword">return</span> v_w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input vector, returns nearest word(s)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vec_sim</span><span class="params">(self, word, top_n)</span>:</span></span><br><span class="line">        v_w1 = self.word_vec(word)</span><br><span class="line">        word_sim = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.v_count):</span><br><span class="line">            <span class="comment"># Find the similary score for each word in vocab</span></span><br><span class="line">            v_w2 = self.w1[i]</span><br><span class="line">            theta_sum = np.dot(v_w1, v_w2)</span><br><span class="line">            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)</span><br><span class="line">            theta = theta_sum / theta_den</span><br><span class="line"></span><br><span class="line">            word = self.index_word[i]</span><br><span class="line">            word_sim[word] = theta</span><br><span class="line"></span><br><span class="line">        words_sorted = sorted(word_sim.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> word, sim <span class="keyword">in</span> words_sorted[:top_n]:</span><br><span class="line">            print(word, sim)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################################</span></span><br><span class="line">settings = &#123;</span><br><span class="line">    <span class="string">'window_size'</span>: <span class="number">2</span>,           <span class="comment"># context window +- center word</span></span><br><span class="line">    <span class="string">'n'</span>: <span class="number">10</span>,                    <span class="comment"># dimensions of word embeddings, also refer to size of hidden layer</span></span><br><span class="line">    <span class="string">'epochs'</span>: <span class="number">50</span>,               <span class="comment"># number of training epochs</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.01</span>       <span class="comment"># learning rate</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text = <span class="string">"natural language processing and machine learning is fun and exciting"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note the .lower() as upper and lowercase does not matter in our implementation</span></span><br><span class="line"><span class="comment"># [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]</span></span><br><span class="line">corpus = [[word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text.split()]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialise object</span></span><br><span class="line">w2v = word2vec()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numpy ndarray with one-hot representation for [target_word, context_words]</span></span><br><span class="line">training_data = w2v.generate_training_data(settings, corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">w2v.train(training_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get vector for word</span></span><br><span class="line">word = <span class="string">"machine"</span></span><br><span class="line">vec = w2v.word_vec(word)</span><br><span class="line">print(word, vec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find similar words</span></span><br><span class="line">w2v.vec_sim(<span class="string">"machine"</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>


        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/13/word2vec原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/13/word2vec原理/" class="post-title-link" itemprop="url">word2vec原理</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-13 10:16:19" itemprop="dateCreated datePublished" datetime="2019-12-13T10:16:19+08:00">2019-12-13</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-17 17:43:26" itemprop="dateModified" datetime="2019-12-17T17:43:26+08:00">2019-12-17</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>推荐阅读：<br>Jay Alammar图解word2vec: <a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-word2vec/</a><br>word2vec——numpy原生实现: <a href="https://www.jianshu.com/p/b65f884c7237" target="_blank" rel="noopener">https://www.jianshu.com/p/b65f884c7237</a></p>
<h3 id="word2vec是什么？"><a href="#word2vec是什么？" class="headerlink" title="word2vec是什么？"></a>word2vec是什么？</h3><p>Word2vec是用来生成word embeddings的模型，将输入的语料映射到向量空间。这样做有两个好处：一是将文本用代数向量表示，机器能够理解，二则向量之间的相关性可以通过余弦距离计算。</p>
<h3 id="CBOW-and-Skip-grams"><a href="#CBOW-and-Skip-grams" class="headerlink" title="CBOW and Skip-grams"></a>CBOW and Skip-grams</h3><p>CBOW 和 Skip-grams 是word2vec的两种训练模式，产生不同的训练数据。CBOW是用上下文来预测当前词，Skip-grams是用当前词来预测上下文。<br>举个例子，假设输入语料是一句诗“生如夏花之绚烂，死如秋叶之静美。”，采用CBOW和Skip-grams两种方法获得训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"生如夏花之绚烂，死如秋叶之静美。"</span></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line">text_tokenize = nltk.word_tokenize(text)·</span><br><span class="line"><span class="comment"># text_tokenize = ["生","如","夏花","之","绚烂","，","死","如","秋叶","之","静美","。"]</span></span><br></pre></td></tr></table></figure>

<p><strong>CBOW</strong><br>CBOW将产生12个训练样本：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/cbow.png?x-oss-process=style/resize_w_1200" alt="cbow"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/cbow-data.png?x-oss-process=style/resize_w_1200" alt="cbow-data"></p>
<p><strong>Skip-grams</strong><br>Skip-grams将产生42个训练样本：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/skip-grams.png?x-oss-process=style/resize_w_1200" alt="skip-grams"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/skip-grams-data.png?x-oss-process=style/resize_w_1200" alt="skip-grams-data"></p>
<h3 id="负例采样"><a href="#负例采样" class="headerlink" title="负例采样"></a>负例采样</h3><ol>
<li>将预测相邻单词的任务，切换为提取输入与输出单词的模型，并输出一个表明它们是否是邻居的分数，0表示“不是邻居”，1表示“邻居”。这个简单的转变将神经网络改为逻辑回归模型——<strong>加快训练速度</strong>。</li>
<li>添加负样本，输出词从词汇表中随机抽取单词。<br>切换之后训练数据集的结构如下:<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sgns.png?x-oss-process=style/resize_w_1200" alt="sgns"><h3 id="Word2vec训练"><a href="#Word2vec训练" class="headerlink" title="Word2vec训练"></a>Word2vec训练</h3>word2vec训练的的两个核心思想分别是Skip-grams和负例采样。<br>训练开始时，创建两个矩阵——Embedding矩阵和Context矩阵（随机值初始化）。其中，vocab_size表示词典大小，embedding_size表示嵌入长度。<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/word2vec_weights.png?x-oss-process=style/resize_w_1200" alt="word2vec_weights"></li>
</ol>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/05/使用Fairseq训练gec模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/05/使用Fairseq训练gec模型/" class="post-title-link" itemprop="url">使用Fairseq训练gec模型</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-05 16:35:29" itemprop="dateCreated datePublished" datetime="2019-12-05T16:35:29+08:00">2019-12-05</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-25 12:17:46" itemprop="dateModified" datetime="2019-12-25T12:17:46+08:00">2019-12-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/thoughts/" itemprop="url" rel="index"><span itemprop="name">thoughts</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>实现过程参考猿辅导团队和Kakao团队的项目：<br>猿辅导:<a href="https://github.com/zhawe01/fairseq-gec" target="_blank" rel="noopener">https://github.com/zhawe01/fairseq-gec</a><br>Kakao:<a href="https://github.com/kakaobrain/helo_word" target="_blank" rel="noopener">https://github.com/kakaobrain/helo_word</a></p>
<h3 id="Fairseq是什么？"><a href="#Fairseq是什么？" class="headerlink" title="Fairseq是什么？"></a>Fairseq是什么？</h3><p>Fairseq(-py)是Facebook人工智能研究院研发的一个序列建模工具包，使研发人员可以训练自定义模型来进行翻译、摘要、语言建模和其他文本生成任务。它提供了各种序列到序列模型的参考实现，包括长短期记忆（LSTM）网络和新型卷积神经网络（CNN），其生成翻译的速度比相当的递归神经网络（RNN）模型快很多倍。</p>
<h3 id="fairseq-preprocess"><a href="#fairseq-preprocess" class="headerlink" title="fairseq-preprocess"></a>fairseq-preprocess</h3><p>数据预处理：建立词表、训练数据二值化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEXT=&lt;text_path&gt;</span><br><span class="line">fairseq-preprocess --source-lang src --target-lang tgt \</span><br><span class="line">    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \</span><br><span class="line">    --destdir data-bin</span><br></pre></td></tr></table></figure>

<p><strong>Named Arguments</strong><br>argument | default | description<br>–source-lang | | 源语言<br>–target-lang | | 目标语言<br>–trainpref | | 训练数据<br>–validpref | | 验证数据<br>–testpref | | 测试数据<br>–destdir | data-bin | 输出数据存放位置<br>–output-format | binary, raw | 输出数据的格式<br>–src-dict | | 源语言字典<br>–tgt-dict | | 目标语言字典<br>–joined-dictionary | | 联合字典，复制源语言字典为目标语言字典<br>–padding-factor | 8 | 填充字典大小为N的倍数<br>–task | translation | 任务类型<br><strong>举个例子</strong><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/fairseq-preprocess.png?x-oss-process=style/resize_w_1200" alt="fairseq-preprocess"><br>生成的词表、训练数据如下图所示:<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/fairseq_1.png?x-oss-process=style/resize_w_1200" alt="fairseq_1"></p>
<h3 id="fairseq-train"><a href="#fairseq-train" class="headerlink" title="fairseq-train"></a>fairseq-train</h3><h3 id="fairseq-generate"><a href="#fairseq-generate" class="headerlink" title="fairseq-generate"></a>fairseq-generate</h3><p>用训练好的模型翻译预处理后的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DATA_BIN=&lt;data_bin_path&gt;</span><br><span class="line">MODEL_DIR=&lt;model_path&gt;</span><br><span class="line">fairseq-generate $DATA_BIN \</span><br><span class="line">    --path $MODEL_DIR/model.pt \</span><br><span class="line">    --batch-size <span class="number">128</span> --beam <span class="number">5</span></span><br></pre></td></tr></table></figure>

<h3 id="fairseq-interactive"><a href="#fairseq-interactive" class="headerlink" title="fairseq-interactive"></a>fairseq-interactive</h3><p>用训练好的模型翻译原始数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DIR=&lt;model_path&gt;</span><br><span class="line">fairseq-interactive \</span><br><span class="line">    --path $MODEL_DIR/model.pt $MODEL_DIR \</span><br><span class="line">    --beam <span class="number">5</span> --source-lang src --target-lang tgt \</span><br><span class="line">    --tokenizer moses \</span><br><span class="line">    --bpe subword_nmt --bpe-codes $MODEL_DIR/bpecodes</span><br></pre></td></tr></table></figure>

<p><strong>raw-text</strong>指的是分词后的句子，<strong>pre-processed data</strong>指的是bin文件</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/04/Errant在GEC中的应用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/04/Errant在GEC中的应用/" class="post-title-link" itemprop="url">Errant在GEC中的应用</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-04 10:21:43 / Modified: 18:59:54" itemprop="dateCreated datePublished" datetime="2019-12-04T10:21:43+08:00">2019-12-04</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/thoughts/" itemprop="url" rel="index"><span itemprop="name">thoughts</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>errant: <a href="https://github.com/chrisjbryant/errant" target="_blank" rel="noopener">https://github.com/chrisjbryant/errant</a><br>Kakao: <a href="https://github.com/kakaobrain/helo_word" target="_blank" rel="noopener">https://github.com/kakaobrain/helo_word</a></p>
<h3 id="Errant提供的方法"><a href="#Errant提供的方法" class="headerlink" title="Errant提供的方法"></a>Errant提供的方法</h3><p>Errant是一个语法错误标注工具，可以从平行语料中自动提取edits，并进行错误分类。Errant工具提供以下几个方法：</p>
<ol>
<li><code>parallel_to_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 parallel_to_m2.py -orig &lt;orig_file&gt; -cor &lt;cor_file1&gt; [&lt;cor_file2&gt; ...] -out &lt;out_m2&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>从平行语料自动提取edits并进行分类，运行该命令需要一个original文件，及至少一个corrected文件，指定一个输出M2文件名。<strong>original和corrected文件都必须一行一个句子，并且需要分词</strong><br><strong>举个例子</strong><br><strong>Original</strong>: This are gramamtical sentence .<br><strong>Corrected</strong>: This is a grammatical sentence .<br><strong>Output M2</strong>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">S This are gramamtical sentence .  </span><br><span class="line">A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0  </span><br><span class="line">A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>m2_to_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 m2_to_m2.py &#123;-auto|-gold&#125; m2_file -out &lt;out_m2&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>将M2文件转成M2文件。用处在于，有一些M2文件是由专家标注，错误分类和标注方法与Errant不同，此方法可以将这类型的M2转换到Errant格式上，统一错误类型。<br><code>-gold</code>只将现存的edits进行分类<br><code>-auto</code>自动提取edits并且分类<br><strong>conll2014.m2是一个由专家标注的包含1312对平行语料的公开gec任务测试集，使用m2_to_m2的方法，将conll2014.m2转换成conll2014.errant.m2之后，不影响评价结果</strong></p>
<ol start="3">
<li><code>compare_m2.py</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; </span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -cat &#123;1,2,3&#125;</span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -ds</span><br><span class="line">python3 compare_m2.py -hyp &lt;hyp_m2&gt; -ref &lt;ref_m2&gt; -ds -cat &#123;1,2,3&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>比较M2文件，可以用来评估猜想的M2与参考的M2的相似程度。评估结果给出precision、recall、F-score (F 0.5)</p>
<h3 id="Errant在GEC中的应用"><a href="#Errant在GEC中的应用" class="headerlink" title="Errant在GEC中的应用"></a>Errant在GEC中的应用</h3><p>Kakao团队在2019 BEA GEC共享任务中，取得领先成果。<br>参考Kakao团队的方法，在gec中应用errant，对gec系统进行效果评估，控制输出的错误类型，提升精度。工作流程见下图：<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/errant_gec.png?x-oss-process=style/resize_w_1200" alt="gec_errant"><br>图中<strong>conll2014.ori</strong>与<strong>conll2014.gold.m2</strong>为公开测试集的original数据与专家标注的m2文件。流程解读：</p>
<ol>
<li>将<strong>conll2014.ori</strong>数据输入gec系统中，得到预测结果<strong>conll2014.pred</strong></li>
<li>将<strong>conll2014.ori</strong>与<strong>conll2014.pred</strong>作为平行语料，使用<code>parallel_to_m2.py</code>方法，得到<strong>conll2014.pred._m2</strong></li>
<li>使用<code>compare_m2.py</code>比较两个m2文件: <strong>conll2014.pred._m2</strong>和<strong>conll2014.gold.m2</strong>，得到<strong>conll2014.report</strong></li>
<li>使用<code>error_type_control.py</code>分析<strong>conll2014.report</strong>,找到精度低的错误类型</li>
<li>除去精度低的错误类型，再输入gec系统中，得到纠正的结果<strong>conll2014.cor</strong></li>
</ol>
<p><code>error_type_control.py</code>是Kakao团队提供的方法，输入report，自动找出精度低的错误类型的组合，输出一个错误类型列表。Errant根据规则制定了25种错误类型，有一些类型并不常见，召回率低，有一些类型常见，但是精度低。通过<code>error_type_control.py</code>过滤掉一些不常见、精度低的错误类型，可以提升整体的表现。</p>
<p><strong>conll2014.report长这样</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">===================== Span-Based Correction ======================</span><br><span class="line">Category       TP       FP       FN       P        R        F0<span class="number">.5</span></span><br><span class="line">M:ADJ          <span class="number">0</span>        <span class="number">1</span>        <span class="number">3</span>        <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">M:ADV          <span class="number">2</span>        <span class="number">1</span>        <span class="number">4</span>        <span class="number">0.6667</span>   <span class="number">0.3333</span>   <span class="number">0.5556</span></span><br><span class="line">M:CONJ         <span class="number">0</span>        <span class="number">1</span>        <span class="number">6</span>        <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">M:DET          <span class="number">73</span>       <span class="number">44</span>       <span class="number">65</span>       <span class="number">0.6239</span>   <span class="number">0.529</span>    <span class="number">0.6023</span></span><br><span class="line">M:NOUN         <span class="number">1</span>        <span class="number">7</span>        <span class="number">13</span>       <span class="number">0.125</span>    <span class="number">0.0714</span>   <span class="number">0.1087</span></span><br><span class="line">M:NOUN:POSS    <span class="number">3</span>        <span class="number">2</span>        <span class="number">5</span>        <span class="number">0.6</span>      <span class="number">0.375</span>    <span class="number">0.5357</span></span><br><span class="line">M:OTHER        <span class="number">2</span>        <span class="number">4</span>        <span class="number">19</span>       <span class="number">0.3333</span>   <span class="number">0.0952</span>   <span class="number">0.2222</span></span><br><span class="line">M:PART         <span class="number">2</span>        <span class="number">3</span>        <span class="number">6</span>        <span class="number">0.4</span>      <span class="number">0.25</span>     <span class="number">0.3571</span></span><br><span class="line">M:PREP         <span class="number">28</span>       <span class="number">22</span>       <span class="number">30</span>       <span class="number">0.56</span>     <span class="number">0.4828</span>   <span class="number">0.5426</span></span><br><span class="line">M:PRON         <span class="number">2</span>        <span class="number">9</span>        <span class="number">10</span>       <span class="number">0.1818</span>   <span class="number">0.1667</span>   <span class="number">0.1786</span></span><br><span class="line">M:PUNCT        <span class="number">43</span>       <span class="number">30</span>       <span class="number">75</span>       <span class="number">0.589</span>    <span class="number">0.3644</span>   <span class="number">0.5244</span></span><br><span class="line">M:VERB         <span class="number">7</span>        <span class="number">11</span>       <span class="number">14</span>       <span class="number">0.3889</span>   <span class="number">0.3333</span>   <span class="number">0.3763</span></span><br><span class="line">M:VERB:FORM    <span class="number">2</span>        <span class="number">1</span>        <span class="number">4</span>        <span class="number">0.6667</span>   <span class="number">0.3333</span>   <span class="number">0.5556</span></span><br><span class="line">M:VERB:TENSE   <span class="number">10</span>       <span class="number">9</span>        <span class="number">14</span>       <span class="number">0.5263</span>   <span class="number">0.4167</span>   <span class="number">0.5</span></span><br><span class="line">R:ADJ          <span class="number">1</span>        <span class="number">11</span>       <span class="number">20</span>       <span class="number">0.0833</span>   <span class="number">0.0476</span>   <span class="number">0.0725</span></span><br><span class="line">R:ADJ:FORM     <span class="number">7</span>        <span class="number">0</span>        <span class="number">4</span>        <span class="number">1.0</span>      <span class="number">0.6364</span>   <span class="number">0.8974</span></span><br><span class="line">R:ADV          <span class="number">2</span>        <span class="number">2</span>        <span class="number">11</span>       <span class="number">0.5</span>      <span class="number">0.1538</span>   <span class="number">0.3448</span></span><br><span class="line">R:CONJ         <span class="number">0</span>        <span class="number">0</span>        <span class="number">6</span>        <span class="number">1.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">R:CONTR        <span class="number">0</span>        <span class="number">0</span>        <span class="number">1</span>        <span class="number">1.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">R:DET          <span class="number">31</span>       <span class="number">39</span>       <span class="number">42</span>       <span class="number">0.4429</span>   <span class="number">0.4247</span>   <span class="number">0.4391</span></span><br><span class="line">R:MORPH        <span class="number">38</span>       <span class="number">15</span>       <span class="number">38</span>       <span class="number">0.717</span>    <span class="number">0.5</span>      <span class="number">0.6597</span></span><br><span class="line">R:NOUN         <span class="number">13</span>       <span class="number">36</span>       <span class="number">68</span>       <span class="number">0.2653</span>   <span class="number">0.1605</span>   <span class="number">0.2347</span></span><br><span class="line">R:NOUN:INFL    <span class="number">5</span>        <span class="number">4</span>        <span class="number">1</span>        <span class="number">0.5556</span>   <span class="number">0.8333</span>   <span class="number">0.5952</span></span><br><span class="line">R:NOUN:NUM     <span class="number">156</span>      <span class="number">66</span>       <span class="number">71</span>       <span class="number">0.7027</span>   <span class="number">0.6872</span>   <span class="number">0.6996</span></span><br><span class="line">R:NOUN:POSS    <span class="number">3</span>        <span class="number">1</span>        <span class="number">7</span>        <span class="number">0.75</span>     <span class="number">0.3</span>      <span class="number">0.5769</span></span><br><span class="line">R:ORTH         <span class="number">32</span>       <span class="number">14</span>       <span class="number">18</span>       <span class="number">0.6957</span>   <span class="number">0.64</span>     <span class="number">0.6838</span></span><br><span class="line">R:OTHER        <span class="number">28</span>       <span class="number">93</span>       <span class="number">268</span>      <span class="number">0.2314</span>   <span class="number">0.0946</span>   <span class="number">0.1795</span></span><br><span class="line">R:PART         <span class="number">17</span>       <span class="number">10</span>       <span class="number">8</span>        <span class="number">0.6296</span>   <span class="number">0.68</span>     <span class="number">0.6391</span></span><br><span class="line">R:PREP         <span class="number">72</span>       <span class="number">56</span>       <span class="number">68</span>       <span class="number">0.5625</span>   <span class="number">0.5143</span>   <span class="number">0.5521</span></span><br><span class="line">R:PRON         <span class="number">12</span>       <span class="number">10</span>       <span class="number">22</span>       <span class="number">0.5455</span>   <span class="number">0.3529</span>   <span class="number">0.4918</span></span><br><span class="line">R:PUNCT        <span class="number">16</span>       <span class="number">13</span>       <span class="number">31</span>       <span class="number">0.5517</span>   <span class="number">0.3404</span>   <span class="number">0.4908</span></span><br><span class="line">R:SPELL        <span class="number">81</span>       <span class="number">24</span>       <span class="number">17</span>       <span class="number">0.7714</span>   <span class="number">0.8265</span>   <span class="number">0.7819</span></span><br><span class="line">R:VERB         <span class="number">17</span>       <span class="number">18</span>       <span class="number">98</span>       <span class="number">0.4857</span>   <span class="number">0.1478</span>   <span class="number">0.3333</span></span><br><span class="line">R:VERB:FORM    <span class="number">68</span>       <span class="number">33</span>       <span class="number">35</span>       <span class="number">0.6733</span>   <span class="number">0.6602</span>   <span class="number">0.6706</span></span><br><span class="line">R:VERB:INFL    <span class="number">1</span>        <span class="number">0</span>        <span class="number">1</span>        <span class="number">1.0</span>      <span class="number">0.5</span>      <span class="number">0.8333</span></span><br><span class="line">R:VERB:SVA     <span class="number">76</span>       <span class="number">29</span>       <span class="number">37</span>       <span class="number">0.7238</span>   <span class="number">0.6726</span>   <span class="number">0.7129</span></span><br><span class="line">R:VERB:TENSE   <span class="number">32</span>       <span class="number">27</span>       <span class="number">98</span>       <span class="number">0.5424</span>   <span class="number">0.2462</span>   <span class="number">0.4372</span></span><br><span class="line">R:WO           <span class="number">8</span>        <span class="number">6</span>        <span class="number">11</span>       <span class="number">0.5714</span>   <span class="number">0.4211</span>   <span class="number">0.5333</span></span><br><span class="line">U:ADJ          <span class="number">0</span>        <span class="number">3</span>        <span class="number">10</span>       <span class="number">0.0</span>      <span class="number">0.0</span>      <span class="number">0.0</span></span><br><span class="line">U:ADV          <span class="number">3</span>        <span class="number">3</span>        <span class="number">12</span>       <span class="number">0.5</span>      <span class="number">0.2</span>      <span class="number">0.3846</span></span><br><span class="line">U:CONJ         <span class="number">1</span>        <span class="number">0</span>        <span class="number">3</span>        <span class="number">1.0</span>      <span class="number">0.25</span>     <span class="number">0.625</span></span><br><span class="line">U:DET          <span class="number">115</span>      <span class="number">77</span>       <span class="number">72</span>       <span class="number">0.599</span>    <span class="number">0.615</span>    <span class="number">0.6021</span></span><br><span class="line">U:NOUN         <span class="number">1</span>        <span class="number">2</span>        <span class="number">18</span>       <span class="number">0.3333</span>   <span class="number">0.0526</span>   <span class="number">0.1613</span></span><br><span class="line">U:NOUN:POSS    <span class="number">1</span>        <span class="number">0</span>        <span class="number">3</span>        <span class="number">1.0</span>      <span class="number">0.25</span>     <span class="number">0.625</span></span><br><span class="line">U:OTHER        <span class="number">3</span>        <span class="number">20</span>       <span class="number">47</span>       <span class="number">0.1304</span>   <span class="number">0.06</span>     <span class="number">0.1056</span></span><br><span class="line">U:PART         <span class="number">2</span>        <span class="number">1</span>        <span class="number">5</span>        <span class="number">0.6667</span>   <span class="number">0.2857</span>   <span class="number">0.5263</span></span><br><span class="line">U:PREP         <span class="number">43</span>       <span class="number">15</span>       <span class="number">29</span>       <span class="number">0.7414</span>   <span class="number">0.5972</span>   <span class="number">0.7072</span></span><br><span class="line">U:PRON         <span class="number">1</span>        <span class="number">2</span>        <span class="number">10</span>       <span class="number">0.3333</span>   <span class="number">0.0909</span>   <span class="number">0.2174</span></span><br><span class="line">U:PUNCT        <span class="number">5</span>        <span class="number">3</span>        <span class="number">10</span>       <span class="number">0.625</span>    <span class="number">0.3333</span>   <span class="number">0.5319</span></span><br><span class="line">U:VERB         <span class="number">8</span>        <span class="number">5</span>        <span class="number">18</span>       <span class="number">0.6154</span>   <span class="number">0.3077</span>   <span class="number">0.5128</span></span><br><span class="line">U:VERB:FORM    <span class="number">4</span>        <span class="number">0</span>        <span class="number">2</span>        <span class="number">1.0</span>      <span class="number">0.6667</span>   <span class="number">0.9091</span></span><br><span class="line">U:VERB:TENSE   <span class="number">12</span>       <span class="number">8</span>        <span class="number">14</span>       <span class="number">0.6</span>      <span class="number">0.4615</span>   <span class="number">0.566</span></span><br><span class="line"></span><br><span class="line">=========== Span-Based Correction ============</span><br><span class="line">TP	FP	FN	Prec	Rec	F0<span class="number">.5</span></span><br><span class="line"><span class="number">1090</span>	<span class="number">791</span>	<span class="number">1502</span>	<span class="number">0.5795</span>	<span class="number">0.4205</span>	<span class="number">0.5388</span></span><br><span class="line">==============================================</span><br></pre></td></tr></table></figure>

<p><strong>在我的项目中，主要注重提升精度，过滤错误类型之后，在conll2014测试集上精度提升3到4个百分点</strong></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/03/linux常用命令/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/03/linux常用命令/" class="post-title-link" itemprop="url">linux常用命令</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-03 16:02:59" itemprop="dateCreated datePublished" datetime="2019-12-03T16:02:59+08:00">2019-12-03</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-23 17:49:30" itemprop="dateModified" datetime="2019-12-23T17:49:30+08:00">2019-12-23</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="linux常用的命令"><a href="#linux常用的命令" class="headerlink" title="linux常用的命令"></a>linux常用的命令</h3><p>从服务器下载数据<br><code>scp -r root@43.224.34.73:/home/lk /root</code></p>
<p>移动文件/文件夹<br><code>mv &lt;source_path&gt; &lt;target_path&gt;</code></p>
<p>拷贝文件/文件夹<br><code>cp -r &lt;source_path&gt; &lt;target_path&gt;</code></p>
<p>查看已安装的包<br><code>pip list</code></p>
<p>查看python版本<br><code>which python</code></p>
<p>切换python版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alias python=&apos;/usr/local/bin/python3.6&apos;</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>将dos文件转成unix格式<br><code>dos2unix &lt;file_path&gt;</code></p>
<p>杀掉进程<br><code>kill -9 {pid}</code></p>
<p>查看显卡使用情况<br><code>nvidia-smi</code></p>
<p>查看top进程<br><code>htop</code><br><code>top</code></p>
<p>解压zip文件<br><code>unzip {file_name}</code></p>
<h3 id="conda常用的命令"><a href="#conda常用的命令" class="headerlink" title="conda常用的命令"></a>conda常用的命令</h3><p>进入环境<br><code>source activate {env_name}</code></p>
<p>退出环境<br><code>conda deactivate</code></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/03/BLEU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/12/03/BLEU/" class="post-title-link" itemprop="url">BLEU</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-03 10:14:01 / Modified: 16:21:07" itemprop="dateCreated datePublished" datetime="2019-12-03T10:14:01+08:00">2019-12-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/note/" itemprop="url" rel="index"><span itemprop="name">note</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="机器翻译评价指标——BLEU算法"><a href="#机器翻译评价指标——BLEU算法" class="headerlink" title="机器翻译评价指标——BLEU算法"></a>机器翻译评价指标——BLEU算法</h3><h3 id="一、BLEU是什么？"><a href="#一、BLEU是什么？" class="headerlink" title="一、BLEU是什么？"></a>一、BLEU是什么？</h3><p>BLEU全名为：bilingual evaluation understudy，即：双语互译质量评估辅助工具。<br><em>机器翻译结果越接近专业人工翻译的结果，则越好。</em><br><strong>一句机器翻译的话与其对应的几个参考翻译作比较，算出一个综合分数，这个分数越高，说明机器翻译得越好。</strong></p>
<h3 id="二、BLEU的优缺点"><a href="#二、BLEU的优缺点" class="headerlink" title="二、BLEU的优缺点"></a>二、BLEU的优缺点</h3><p><strong>优点</strong><br>方便、快捷、参考价值<br><strong>缺点</strong></p>
<ol>
<li>不考虑语法准确性；</li>
<li>测评精度收到常用词干扰；</li>
<li>短译句的测评精度有时会较高；</li>
<li>没有考虑同义词或相似表达的情况，导致合理的翻译被否定。</li>
</ol>
<h3 id="三、BLEU的算法"><a href="#三、BLEU的算法" class="headerlink" title="三、BLEU的算法"></a>三、BLEU的算法</h3><p>实现BLEU算法需要以下两个工具：</p>
<ol>
<li>衡量机器翻译结果接近人工翻译结果的数值指标；</li>
<li>一套人工翻译的高质量参考译文。</li>
</ol>
<p><strong>多元精度</strong><br>n-gram精度得分可以衡量翻译评估的充分性和流畅性两个指标：<br>一元组属于字符级别，提现翻译的充分性，衡量逐字翻译能力；多元组是词汇级别，提现翻译的流畅性，词组准了，说话自然相对流畅了。<br><strong>组合多元精度</strong><br>采取几何加权平均，将各个n-gram的作用视为同等重要，权重服从均匀分布。<br><strong>短译句惩罚</strong><br>短译句容易得高分，BP代表译句较短惩罚值。</p>
<h3 id="四、实现"><a href="#四、实现" class="headerlink" title="四、实现"></a>四、实现</h3><p>在nltk的工具包中实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> sentence_bleu, corpus_bleu</span><br><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> SmoothingFunction</span><br><span class="line"></span><br><span class="line">reference = [[<span class="string">'The'</span>, <span class="string">'cat'</span>, <span class="string">'is'</span>, <span class="string">'on'</span>, <span class="string">'the'</span>, <span class="string">'mat'</span>]]</span><br><span class="line">candidate = [<span class="string">'The'</span>, <span class="string">'cat'</span>, <span class="string">'sat'</span>, <span class="string">'on'</span>, <span class="string">'the'</span>, <span class="string">'mat'</span>]</span><br><span class="line"></span><br><span class="line">smooth = SmoothingFunction() <span class="comment"># 定义平滑对象</span></span><br><span class="line">score = sentence_bleu(reference, candidate, weights=(<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>), smoothing_function=smooth.method1)</span><br><span class="line">corpus_score = corpus_bleu([reference], [candidate], smoothing_function=smooth.method1)</span><br><span class="line">print(<span class="string">'score: '</span>, score)</span><br><span class="line">print(<span class="string">'corpus_score: '</span>, corpus_score)</span><br><span class="line"><span class="comment"># score: 0.25406637407730737</span></span><br><span class="line"><span class="comment"># corpus_score: 0.25406637407730737</span></span><br></pre></td></tr></table></figure>
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/18/又是一个美丽的日落/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/18/又是一个美丽的日落/" class="post-title-link" itemprop="url">一个美丽的日落</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-11-18 17:25:20 / Modified: 17:51:18" itemprop="dateCreated datePublished" datetime="2019-11-18T17:25:20+08:00">2019-11-18</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/photos/" itemprop="url" rel="index"><span itemprop="name">photos</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sunset_1030_1.jpg?x-oss-process=style/resize_w_1200" alt="sunset"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sunset_1030_2.jpg?x-oss-process=style/resize_w_1200" alt="sunset"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sunset_1030_3.jpg?x-oss-process=style/resize_w_1200" alt="sunset"><br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/sunset_1030_4.jpg?x-oss-process=style/resize_w_1200" alt="sunset"></p>
<p>我们都记得这个夕阳染红天的日子<br>看一眼这美丽的夕阳，被踹一脚也是值得的<br>希望每天都有美丽的夕阳，慰藉你疲惫的身心<br>从军营出来的你，一定是一名好的战士</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/18/好看的滤镜/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/11/18/好看的滤镜/" class="post-title-link" itemprop="url">好看的滤镜</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-11-18 17:16:13 / Modified: 17:21:13" itemprop="dateCreated datePublished" datetime="2019-11-18T17:16:13+08:00">2019-11-18</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/photos/" itemprop="url" rel="index"><span itemprop="name">photos</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>突然发现一个好看的滤镜<br><img src="https://ln-github-blog.oss-cn-shenzhen.aliyuncs.com/filter_grass.jpg?x-oss-process=style/resize_w_1200" alt="roof sunset"></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/09/在GEC中合并伪数据的实证研究/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
            
            <a href="/2019/10/09/在GEC中合并伪数据的实证研究/" class="post-title-link" itemprop="url">在GEC中合并伪数据的实证研究</a>
          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-10-09 15:12:25" itemprop="dateCreated datePublished" datetime="2019-10-09T15:12:25+08:00">2019-10-09</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-11 10:56:53" itemprop="dateModified" datetime="2019-10-11T10:56:53+08:00">2019-10-11</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="在GEC中合并伪数据的实证研究"><a href="#在GEC中合并伪数据的实证研究" class="headerlink" title="在GEC中合并伪数据的实证研究"></a>在GEC中合并伪数据的实证研究</h3><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>至今，许多研究把语法纠错当做机器翻译的任务，其中，不符合语法规则的句子被当做源语言，符合语法规则的句子被当做目标语言。这个方法允许采用最新的神经MT(machine translation)模型。举个例子，encoder-decoder模型——最初为MT提出的，广泛的应用在GEC中并且在GEC领域取得了显著的成果。<br>然而，在GEC中应用EncDec的挑战在于，EncDec需要大量的训练数据。GEC中最大的公开平行语料（Lang-8）仅有两百万个句子对。因此，有大量的研究——通过增广的方式将伪数据合并到训练数据中。<br>合并伪数据时，需要考虑一下几项实验配置：(i) 生成伪数据的方法, (ii) 伪数据的种子语料库, (iii) 优化设定。然而，在GEC研究领域，关于这些决策的一致性并未达成。</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="LNdoremi">
  <p class="site-author-name" itemprop="name">LNdoremi</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/LNdoremi" title="GitHub &rarr; https://github.com/LNdoremi" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/liuna_sysu@qq.com" title="E-Mail &rarr; liuna_sysu@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.archao.me/" title="https://www.archao.me/" rel="noopener" target="_blank">Chao</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LNdoremi</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  

  

</body>
</html>
