<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010">
<meta name="keywords" content="nlp,word2vec">
<meta property="og:type" content="article">
<meta property="og:title" content="word2vec的实现">
<meta property="og:url" content="http://yoursite.com/2019/12/17/word2vec的实现/index.html">
<meta property="og:site_name" content="LNdoremi">
<meta property="og:description" content="完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-12-17T10:24:44.477Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="word2vec的实现">
<meta name="twitter:description" content="完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010">
  <link rel="canonical" href="http://yoursite.com/2019/12/17/word2vec的实现/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>word2vec的实现 | LNdoremi</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LNdoremi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/17/word2vec的实现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LNdoremi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LNdoremi">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">word2vec的实现

          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-17 18:23:26 / Modified: 18:24:44" itemprop="dateCreated datePublished" datetime="2019-12-17T18:23:26+08:00">2019-12-17</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This is a implementation of Word2Vec using numpy. Uncomment the print functions to see Word2Vec in action! Also remember to change the number of epochs and set training_data to training_data[0] to avoid flooding your terminal. A Google Sheet implementation of Word2Vec is also available here - https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing</span></span><br><span class="line"><span class="string">Have fun learning!</span></span><br><span class="line"><span class="string">Author: Derek Chia</span></span><br><span class="line"><span class="string">Email: derek@derekchia.com</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment">## Randomly initialise</span></span><br><span class="line">getW1 = [[<span class="number">0.236</span>, <span class="number">-0.962</span>, <span class="number">0.686</span>, <span class="number">0.785</span>, <span class="number">-0.454</span>, <span class="number">-0.833</span>, <span class="number">-0.744</span>, <span class="number">0.677</span>, <span class="number">-0.427</span>, <span class="number">-0.066</span>],</span><br><span class="line">        [<span class="number">-0.907</span>, <span class="number">0.894</span>, <span class="number">0.225</span>, <span class="number">0.673</span>, <span class="number">-0.579</span>, <span class="number">-0.428</span>, <span class="number">0.685</span>, <span class="number">0.973</span>, <span class="number">-0.070</span>, <span class="number">-0.811</span>],</span><br><span class="line">        [<span class="number">-0.576</span>, <span class="number">0.658</span>, <span class="number">-0.582</span>, <span class="number">-0.112</span>, <span class="number">0.662</span>, <span class="number">0.051</span>, <span class="number">-0.401</span>, <span class="number">-0.921</span>, <span class="number">-0.158</span>, <span class="number">0.529</span>],</span><br><span class="line">        [<span class="number">0.517</span>, <span class="number">0.436</span>, <span class="number">0.092</span>, <span class="number">-0.835</span>, <span class="number">-0.444</span>, <span class="number">-0.905</span>, <span class="number">0.879</span>, <span class="number">0.303</span>, <span class="number">0.332</span>, <span class="number">-0.275</span>],</span><br><span class="line">        [<span class="number">0.859</span>, <span class="number">-0.890</span>, <span class="number">0.651</span>, <span class="number">0.185</span>, <span class="number">-0.511</span>, <span class="number">-0.456</span>, <span class="number">0.377</span>, <span class="number">-0.274</span>, <span class="number">0.182</span>, <span class="number">-0.237</span>],</span><br><span class="line">        [<span class="number">0.368</span>, <span class="number">-0.867</span>, <span class="number">-0.301</span>, <span class="number">-0.222</span>, <span class="number">0.630</span>, <span class="number">0.808</span>, <span class="number">0.088</span>, <span class="number">-0.902</span>, <span class="number">-0.450</span>, <span class="number">-0.408</span>],</span><br><span class="line">        [<span class="number">0.728</span>, <span class="number">0.277</span>, <span class="number">0.439</span>, <span class="number">0.138</span>, <span class="number">-0.943</span>, <span class="number">-0.409</span>, <span class="number">0.687</span>, <span class="number">-0.215</span>, <span class="number">-0.807</span>, <span class="number">0.612</span>],</span><br><span class="line">        [<span class="number">0.593</span>, <span class="number">-0.699</span>, <span class="number">0.020</span>, <span class="number">0.142</span>, <span class="number">-0.638</span>, <span class="number">-0.633</span>, <span class="number">0.344</span>, <span class="number">0.868</span>, <span class="number">0.913</span>, <span class="number">0.429</span>],</span><br><span class="line">        [<span class="number">0.447</span>, <span class="number">-0.810</span>, <span class="number">-0.061</span>, <span class="number">-0.495</span>, <span class="number">0.794</span>, <span class="number">-0.064</span>, <span class="number">-0.817</span>, <span class="number">-0.408</span>, <span class="number">-0.286</span>, <span class="number">0.149</span>]]</span><br><span class="line"></span><br><span class="line">getW2 = [[<span class="number">-0.868</span>, <span class="number">-0.406</span>, <span class="number">-0.288</span>, <span class="number">-0.016</span>, <span class="number">-0.560</span>, <span class="number">0.179</span>, <span class="number">0.099</span>, <span class="number">0.438</span>, <span class="number">-0.551</span>],</span><br><span class="line">        [<span class="number">-0.395</span>, <span class="number">0.890</span>, <span class="number">0.685</span>, <span class="number">-0.329</span>, <span class="number">0.218</span>, <span class="number">-0.852</span>, <span class="number">-0.919</span>, <span class="number">0.665</span>, <span class="number">0.968</span>],</span><br><span class="line">        [<span class="number">-0.128</span>, <span class="number">0.685</span>, <span class="number">-0.828</span>, <span class="number">0.709</span>, <span class="number">-0.420</span>, <span class="number">0.057</span>, <span class="number">-0.212</span>, <span class="number">0.728</span>, <span class="number">-0.690</span>],</span><br><span class="line">        [<span class="number">0.881</span>, <span class="number">0.238</span>, <span class="number">0.018</span>, <span class="number">0.622</span>, <span class="number">0.936</span>, <span class="number">-0.442</span>, <span class="number">0.936</span>, <span class="number">0.586</span>, <span class="number">-0.020</span>],</span><br><span class="line">        [<span class="number">-0.478</span>, <span class="number">0.240</span>, <span class="number">0.820</span>, <span class="number">-0.731</span>, <span class="number">0.260</span>, <span class="number">-0.989</span>, <span class="number">-0.626</span>, <span class="number">0.796</span>, <span class="number">-0.599</span>],</span><br><span class="line">        [<span class="number">0.679</span>, <span class="number">0.721</span>, <span class="number">-0.111</span>, <span class="number">0.083</span>, <span class="number">-0.738</span>, <span class="number">0.227</span>, <span class="number">0.560</span>, <span class="number">0.929</span>, <span class="number">0.017</span>],</span><br><span class="line">        [<span class="number">-0.690</span>, <span class="number">0.907</span>, <span class="number">0.464</span>, <span class="number">-0.022</span>, <span class="number">-0.005</span>, <span class="number">-0.004</span>, <span class="number">-0.425</span>, <span class="number">0.299</span>, <span class="number">0.757</span>],</span><br><span class="line">        [<span class="number">-0.054</span>, <span class="number">0.397</span>, <span class="number">-0.017</span>, <span class="number">-0.563</span>, <span class="number">-0.551</span>, <span class="number">0.465</span>, <span class="number">-0.596</span>, <span class="number">-0.413</span>, <span class="number">-0.395</span>],</span><br><span class="line">        [<span class="number">-0.838</span>, <span class="number">0.053</span>, <span class="number">-0.160</span>, <span class="number">-0.164</span>, <span class="number">-0.671</span>, <span class="number">0.140</span>, <span class="number">-0.149</span>, <span class="number">0.708</span>, <span class="number">0.425</span>],</span><br><span class="line">        [<span class="number">0.096</span>, <span class="number">-0.995</span>, <span class="number">-0.313</span>, <span class="number">0.881</span>, <span class="number">-0.402</span>, <span class="number">-0.631</span>, <span class="number">-0.660</span>, <span class="number">0.184</span>, <span class="number">0.487</span>]]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">word2vec</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.n = settings[<span class="string">'n'</span>]</span><br><span class="line">        self.lr = settings[<span class="string">'learning_rate'</span>]</span><br><span class="line">        self.epochs = settings[<span class="string">'epochs'</span>]</span><br><span class="line">        self.window = settings[<span class="string">'window_size'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_training_data</span><span class="params">(self, settings, corpus)</span>:</span></span><br><span class="line">        <span class="comment"># Find unique word counts using dictonary</span></span><br><span class="line">        word_counts = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> corpus:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> row:</span><br><span class="line">                word_counts[word] += <span class="number">1</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(word_counts)                                                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # defaultdict(&lt;class 'int'&gt;, &#123;'natural': 1, 'language': 1, 'processing': 1, 'and': 2, 'machine': 1, 'learning': 1, 'is': 1, 'fun': 1, 'exciting': 1&#125;) #</span></span><br><span class="line">        <span class="comment">#########################################################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## How many unique words in vocab? 9</span></span><br><span class="line">        self.v_count = len(word_counts.keys())</span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line">        <span class="comment"># print(self.v_count)   #</span></span><br><span class="line">        <span class="comment"># 9                     #</span></span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate Lookup Dictionaries (vocab)</span></span><br><span class="line">        self.words_list = list(word_counts.keys())</span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.words_list)                                                                        #</span></span><br><span class="line">        <span class="comment"># ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting']  #</span></span><br><span class="line">        <span class="comment">#################################################################################################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate word:index</span></span><br><span class="line">        self.word_index = dict((word, i) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.word_index)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># # &#123;'natural': 0, 'language': 1, 'processing': 2, 'and': 3, 'machine': 4, 'learning': 5, 'is': 6, 'fun': 7, 'exciting': 8&#125; #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate index:word</span></span><br><span class="line">        self.index_word = dict((i, word) <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.words_list))</span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line">        <span class="comment"># print(self.index_word)                                                                                                    #</span></span><br><span class="line">        <span class="comment"># &#123;0: 'natural', 1: 'language', 2: 'processing', 3: 'and', 4: 'machine', 5: 'learning', 6: 'is', 7: 'fun', 8: 'exciting'&#125;   #</span></span><br><span class="line">        <span class="comment">#############################################################################################################################</span></span><br><span class="line"></span><br><span class="line">        training_data = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cycle through each sentence in corpus</span></span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus:</span><br><span class="line">            sent_len = len(sentence)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Cycle through each word in sentence</span></span><br><span class="line">            <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">                <span class="comment"># Convert target word to one-hot</span></span><br><span class="line">                w_target = self.word2onehot(sentence[i])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Cycle through context window</span></span><br><span class="line">                w_context = []</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Note: window_size 2 will have range of 5 values</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(i - self.window, i + self.window+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># Criteria for context word </span></span><br><span class="line">                    <span class="comment"># 1. Target word cannot be context word (j != i)</span></span><br><span class="line">                    <span class="comment"># 2. Index must be greater or equal than 0 (j &gt;= 0) - if not list index out of range</span></span><br><span class="line">                    <span class="comment"># 3. Index must be less or equal than length of sentence (j &lt;= sent_len-1) - if not list index out of range </span></span><br><span class="line">                    <span class="keyword">if</span> j != i <span class="keyword">and</span> j &lt;= sent_len<span class="number">-1</span> <span class="keyword">and</span> j &gt;= <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># Append the one-hot representation of word to w_context</span></span><br><span class="line">                        w_context.append(self.word2onehot(sentence[j]))</span><br><span class="line">                        <span class="comment"># print(sentence[i], sentence[j]) </span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        <span class="comment"># Example:              #</span></span><br><span class="line">                        <span class="comment"># natural language      #</span></span><br><span class="line">                        <span class="comment"># natural processing    #</span></span><br><span class="line">                        <span class="comment"># language natural      #</span></span><br><span class="line">                        <span class="comment"># language processing   #</span></span><br><span class="line">                        <span class="comment"># language append       #</span></span><br><span class="line">                        <span class="comment">#########################</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># training_data contains a one-hot representation of the target word and context words</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                <span class="comment"># Example:                                                                                      #</span></span><br><span class="line">                <span class="comment"># [Target] natural, [Context] language, [Context] processing                                    #</span></span><br><span class="line">                <span class="comment"># print(training_data)                                                                          #</span></span><br><span class="line">                <span class="comment"># [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]]]   #</span></span><br><span class="line">                <span class="comment">#################################################################################################</span></span><br><span class="line">                training_data.append([w_target, w_context])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.array(training_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2onehot</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="comment"># word_vec - initialise a blank vector</span></span><br><span class="line">        word_vec = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.v_count)] <span class="comment"># Alternative - np.zeros(self.v_count)</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line">        <span class="comment"># print(word_vec)           #</span></span><br><span class="line">        <span class="comment"># [0, 0, 0, 0, 0, 0, 0, 0]  #</span></span><br><span class="line">        <span class="comment">#############################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get ID of word from word_index</span></span><br><span class="line">        word_index = self.word_index[word]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Change value from 0 to 1 according to ID of the word</span></span><br><span class="line">        word_vec[word_index] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_data)</span>:</span></span><br><span class="line">        <span class="comment"># Initialising weight matrices</span></span><br><span class="line">        <span class="comment"># np.random.uniform(HIGH, LOW, OUTPUT_SHAPE)</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.uniform.html</span></span><br><span class="line">        self.w1 = np.array(getW1)</span><br><span class="line">        self.w2 = np.array(getW2)</span><br><span class="line">        <span class="comment"># self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))</span></span><br><span class="line">        <span class="comment"># self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cycle through each epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.epochs):</span><br><span class="line">            <span class="comment"># Intialise loss to 0</span></span><br><span class="line">            self.loss = <span class="number">0</span></span><br><span class="line">            <span class="comment"># Cycle through each training sample</span></span><br><span class="line">            <span class="comment"># w_t = vector for target word, w_c = vectors for context words</span></span><br><span class="line">            <span class="keyword">for</span> w_t, w_c <span class="keyword">in</span> training_data:</span><br><span class="line">                <span class="comment"># Forward pass</span></span><br><span class="line">                <span class="comment"># 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)</span></span><br><span class="line">                y_pred, h, u = self.forward_pass(w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment"># print("Vector for target word:", w_t) #</span></span><br><span class="line">                <span class="comment"># print("W1-before backprop", self.w1)  #</span></span><br><span class="line">                <span class="comment"># print("W2-before backprop", self.w2)  #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate error</span></span><br><span class="line">                <span class="comment"># 1. For a target word, calculate difference between y_pred and each of the context words</span></span><br><span class="line">                <span class="comment"># 2. Sum up the differences using np.sum to give us the error for this particular target word</span></span><br><span class="line">                EI = np.sum([np.subtract(y_pred, word) <span class="keyword">for</span> word <span class="keyword">in</span> w_c], axis=<span class="number">0</span>)</span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line">                <span class="comment"># print("Error", EI)    #</span></span><br><span class="line">                <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Backpropagation</span></span><br><span class="line">                <span class="comment"># We use SGD to backpropagate errors - calculate loss on the output layer </span></span><br><span class="line">                self.backprop(EI, h, w_t)</span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line">                <span class="comment">#print("W1-after backprop", self.w1)    #</span></span><br><span class="line">                <span class="comment">#print("W2-after backprop", self.w2)    #</span></span><br><span class="line">                <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Calculate loss</span></span><br><span class="line">                <span class="comment"># There are 2 parts to the loss function</span></span><br><span class="line">                <span class="comment"># Part 1: -ve sum of all the output +</span></span><br><span class="line">                <span class="comment"># Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)</span></span><br><span class="line">                <span class="comment"># Note: word.index(1) returns the index in the context word vector with value 1</span></span><br><span class="line">                <span class="comment"># Note: u[word.index(1)] returns the value of the output layer before softmax</span></span><br><span class="line">                self.loss += -np.sum([u[word.index(<span class="number">1</span>)] <span class="keyword">for</span> word <span class="keyword">in</span> w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">                <span class="comment"># Break if you want to see weights after first target word  #</span></span><br><span class="line">                <span class="comment"># break                                                     #</span></span><br><span class="line">                <span class="comment">#############################################################</span></span><br><span class="line">            print(<span class="string">'Epoch:'</span>, i, <span class="string">"Loss:"</span>, self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_pass</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># x is one-hot vector for target word, shape - 9x1</span></span><br><span class="line">        <span class="comment"># Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1</span></span><br><span class="line">        h = np.dot(x, self.w1)</span><br><span class="line">        <span class="comment"># Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1</span></span><br><span class="line">        u = np.dot(h, self.w2)</span><br><span class="line">        <span class="comment"># Run 1x9 through softmax to force each element to range of [0, 1] - 1x8</span></span><br><span class="line">        y_c = self.softmax(u)</span><br><span class="line">        <span class="keyword">return</span> y_c, h, u</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        e_x = np.exp(x - np.max(x))</span><br><span class="line">        <span class="keyword">return</span> e_x / e_x.sum(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, e, h, x)</span>:</span></span><br><span class="line">        <span class="comment"># https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html</span></span><br><span class="line">        <span class="comment"># Column vector EI represents row-wise sum of prediction errors across each context word for the current center word</span></span><br><span class="line">        <span class="comment"># Going backwards, we need to take derivative of E with respect of w2</span></span><br><span class="line">        <span class="comment"># h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9</span></span><br><span class="line">        <span class="comment"># x - shape 9x1, w2 - 10x9, e.T - 9x1</span></span><br><span class="line">        dl_dw2 = np.outer(h, e)</span><br><span class="line">        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))</span><br><span class="line">        <span class="comment">########################################</span></span><br><span class="line">        <span class="comment"># print('Delta for w2', dl_dw2)         #</span></span><br><span class="line">        <span class="comment"># print('Hidden layer', h)              #</span></span><br><span class="line">        <span class="comment"># print('np.dot', np.dot(self.w2, e.T)) #</span></span><br><span class="line">        <span class="comment"># print('Delta for w1', dl_dw1)         #</span></span><br><span class="line">        <span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update weights</span></span><br><span class="line">        self.w1 = self.w1 - (self.lr * dl_dw1)</span><br><span class="line">        self.w2 = self.w2 - (self.lr * dl_dw2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get vector from word</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_vec</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        w_index = self.word_index[word]</span><br><span class="line">        v_w = self.w1[w_index]</span><br><span class="line">        <span class="keyword">return</span> v_w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Input vector, returns nearest word(s)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vec_sim</span><span class="params">(self, word, top_n)</span>:</span></span><br><span class="line">        v_w1 = self.word_vec(word)</span><br><span class="line">        word_sim = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.v_count):</span><br><span class="line">            <span class="comment"># Find the similary score for each word in vocab</span></span><br><span class="line">            v_w2 = self.w1[i]</span><br><span class="line">            theta_sum = np.dot(v_w1, v_w2)</span><br><span class="line">            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)</span><br><span class="line">            theta = theta_sum / theta_den</span><br><span class="line"></span><br><span class="line">            word = self.index_word[i]</span><br><span class="line">            word_sim[word] = theta</span><br><span class="line"></span><br><span class="line">        words_sorted = sorted(word_sim.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> word, sim <span class="keyword">in</span> words_sorted[:top_n]:</span><br><span class="line">            print(word, sim)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################################</span></span><br><span class="line">settings = &#123;</span><br><span class="line">    <span class="string">'window_size'</span>: <span class="number">2</span>,           <span class="comment"># context window +- center word</span></span><br><span class="line">    <span class="string">'n'</span>: <span class="number">10</span>,                    <span class="comment"># dimensions of word embeddings, also refer to size of hidden layer</span></span><br><span class="line">    <span class="string">'epochs'</span>: <span class="number">50</span>,               <span class="comment"># number of training epochs</span></span><br><span class="line">    <span class="string">'learning_rate'</span>: <span class="number">0.01</span>       <span class="comment"># learning rate</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text = <span class="string">"natural language processing and machine learning is fun and exciting"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note the .lower() as upper and lowercase does not matter in our implementation</span></span><br><span class="line"><span class="comment"># [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]</span></span><br><span class="line">corpus = [[word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text.split()]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialise object</span></span><br><span class="line">w2v = word2vec()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numpy ndarray with one-hot representation for [target_word, context_words]</span></span><br><span class="line">training_data = w2v.generate_training_data(settings, corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">w2v.train(training_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get vector for word</span></span><br><span class="line">word = <span class="string">"machine"</span></span><br><span class="line">vec = w2v.word_vec(word)</span><br><span class="line">print(word, vec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find similar words</span></span><br><span class="line">w2v.vec_sim(<span class="string">"machine"</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>


    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/nlp/" rel="tag"># nlp</a>
            
              <a href="/tags/word2vec/" rel="tag"># word2vec</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/12/13/word2vec原理/" rel="next" title="word2vec原理">
                  <i class="fa fa-chevron-left"></i> word2vec原理
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/12/23/pytorch-tensor-常用运算/" rel="prev" title="pytorch-tensor-常用运算">
                  pytorch-tensor-常用运算 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="LNdoremi">
  <p class="site-author-name" itemprop="name">LNdoremi</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/LNdoremi" title="GitHub &rarr; https://github.com/LNdoremi" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/liuna_sysu@qq.com" title="E-Mail &rarr; liuna_sysu@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.archao.me/" title="https://www.archao.me/" rel="noopener" target="_blank">Chao</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LNdoremi</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  

  

</body>
</html>
